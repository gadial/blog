---
id: 339
title: "אז מהי השערת רימן?"
date: 2010-02-08 09:53:19
layout: post
categories: 
  - אנליזה מתמטית
  - בעיות מתמטיות מפורסמות
  - ההיסטוריה של המתמטיקה
  - תורת המספרים
tags: 
  - השערת רימן
  - משפט המספרים הראשוניים
  - פונקציות מרוכבות
  - פונקצית הזטא של רימן
social_media_share: true
---
בנובמבר 1859, לפני קצת יותר ממאה וחמישים שנים, פרסם המתמטיקאי ברנרד רימן מאמר קצרצר, עשרה עמודים אורכו (או שמונה, תלוי את מי שואלים), שפחות או יותר בישר את לידת <a href="http://en.wikipedia.org/wiki/Analytic_number_theory">תורת המספרים האנליטית</a> והכיל בתוכו, כמעט בדרך אגב, את מה שהפכה להיות הבעיה הפתוחה המפורסמת ביותר במתמטיקה - השערת רימן. לכבוד "יום ההולדת" של 150 שנים שהיה לא מזמן להשערה אני מרגיש מחוייב לכתוב פוסט שמסביר בערך מה אומרת ההשערה, ואולי גם טיפה למה היא מעניינת כל כך, אבל לפחות הפעם לא ארחיב יותר מדי (ויש הרבה על מה להרחיב כאן); בתקווה, בעתיד יבוא תיאור הרבה יותר מפורט. להשערת רימן אין את אותו מזל כמו של <a href="http://he.wikipedia.org/wiki/%D7%94%D7%9E%D7%A9%D7%A4%D7%98_%D7%94%D7%90%D7%97%D7%A8%D7%95%D7%9F_%D7%A9%D7%9C_%D7%A4%D7%A8%D7%9E%D7%94">המשפט האחרון של פרמה</a> או <a href="http://he.wikipedia.org/wiki/%D7%94%D7%A9%D7%A2%D7%A8%D7%AA_%D7%92%D7%95%D7%9C%D7%93%D7%91%D7%9A">השערת גולדבך</a> - הניסוח שלה אינו פשוט והיא אינה אינטואיטיבית או פשוטה למראה כמותם, ולכן מיעוט יחסי של אנשים - גם בקרב המתעניינים במתמטיקה - בקיא בניסוח המדוייק שלה. אני מקווה שהפוסט הזה יוכל לסייע מעט למצב העניינים הזה.

אתחיל מהמוטיבציה, שבלעדיה התגובה הראשונה להשערת רימן היא "למי אכפת?". רימן התעניין ב<a href="http://he.wikipedia.org/wiki/%D7%9E%D7%A1%D7%A4%D7%A8_%D7%A8%D7%90%D7%A9%D7%95%D7%A0%D7%99">מספרים הראשוניים</a>, "אבני הבניין" של כל שאר המספרים הטבעיים. מאז ומעולם חיפשו המתמטיקאים נוסחה פשוטה שתניב מספרים ראשוניים, אך מכיוון שזה קשה עד בלתי אפשרי למצוא נוסחה שכזו (תלוי למה בדיוק מתכוונים ב"נוסחה") עיקר המאמץ מתמקד בנסיון להבין את הפיזור היחסי של הראשוניים. למשל, אם ניקח את המספר {% equation %}x{% endequation %}, כמה מספרים ראשוניים קיימים שקטנים מ-{% equation %}x{% endequation %}? מספר זה מסומן ב-{% equation %}\pi\left(x\right){% endequation %} וכאמור - קשה לחשב אותו במדוייק, אך ניתן לנסות ולתת הערכות. זה בדיוק מה שרימן ניסה לעשות במאמרו, שכותרתו היא "על מספר הראשוניים הקטנים מכמות מסויימת". לא אכנס כרגע לטיב ההערכה הספציפית שרימן מצא, כי בקרוב אדבר על אחרות, מדוייקות יותר; מה שהיה כל כך חשוב ופורץ דרך במאמר של רימן היה הכלי שבו הוא בחר לתקוף את הבעיה, כלי שלא השתמשו בו קודם באופן כל כך ברור וחד משמעי - <a href="http://he.wikipedia.org/wiki/%D7%90%D7%A0%D7%9C%D7%99%D7%96%D7%94_%D7%9E%D7%A8%D7%95%D7%9B%D7%91%D7%AA">האנליזה המרוכבת</a>, ובפרט פונקציה מסויימת שבזכות מאמר זה זכתה לשם "<a href="http://he.wikipedia.org/wiki/%D7%A4%D7%95%D7%A0%D7%A7%D7%A6%D7%99%D7%99%D7%AA_%D7%96%D7%98%D7%90_%D7%A9%D7%9C_%D7%A8%D7%99%D7%9E%D7%9F">פונקצית הזטא של רימן</a>" (למה "זטא"? ובכן, כי זו האות היוונית שבה רימן השתמש כדי לסמן את הפונקציה; מאז רימן המונח "פונקצית זטא" השתרש כדי לתאר פונקציות שבמובן מסויים דומות לפונקציה של רימן, אך כאן זה ממש לא הזמן להיכנס לכך).

בואו נחזור קצת אחורה בזמן, אל אוילר. אוילר התעסק עם <a href="http://www.gadial.net/2008/06/17/infinite_series/">הטור</a> {% equation %}\sum_{n=1}^{\infty}\frac{1}{n^{s}}{% endequation %}. זה טור מסקרן למדי - אם {% equation %}s&gt;1{% endequation %} הוא מתכנס (ואוילר הוכיח כי עבור {% equation %}s=2{% endequation %} סכומו הוא {% equation %}\frac{\pi^{2}}{6}{% endequation %}) ואם {% equation %}s\le1{% endequation %} הוא מתבדר (כלומר, גדל ועוד ועוד עד לאינסוף). אבל מה שבאמת היה מעניין בטור הזה הוא דרך הצגה שונה שלו, שאוילר גילה - בתור מכפלה אינסופית: {% equation %}\sum_{n=1}^{\infty}\frac{1}{n^{s}}=\prod_{p}\frac{1}{1-p^{-s}}{% endequation %}, כאשר המכפלה נלקחת על כל המספרים <strong>הראשוניים</strong> {% equation %}p{% endequation %}. הסיבה לנכונות השוויון היא המשפט היסודי של האריתמטיקה - כל מספר {% equation %}n{% endequation %} ניתן להציג בצורה <strong>יחידה</strong> כמכפלת ראשוניים. על האופן המדוייק שבו השוויון מתקבל <a href="http://www.gadial.net/2009/05/17/euler_proof_infinity_of_primes/">כבר הרחבתי בעבר</a> ולא אחזור על כך - הנקודה החשובה כאן היא שהשוויון הזה הוא בעצם דרך <strong>אנליטית</strong> לתאר את התוצאה הבסיסית ביותר של תורת המספרים.

מכיוון שעבור ערכים שונים של {% equation %}s{% endequation %} הטור (והמכפלה) מניבים ערכים שונים, יש הגיון בדיבור על הטור הזה בתור פונקציה של {% equation %}s{% endequation %}; נשתמש בסימון {% equation %}\zeta\left(s\right){% endequation %} (איני בטוח אם רימן המציא את הסימון הזה או לא, וכמובן שזה לא חשוב מדי). כאמור, הפונקציה הזו לא מוגדרת בכלל עבור ערכים קטנים או שווים ל-1, ובוודאי שאינה מוגדרת עבור מספרים מרוכבים. הדבר הראשון שעשה רימן במאמרו היה לטפל בדיוק בעניין הזה - הוא הרחיב את {% equation %}\zeta\left(s\right){% endequation %} לכל המספרים המרוכבים. יותר במדוייק, הוא הראה שקיימת פונקציה מרוכבת יחידה שהיא בעלת תכונות "נחמדות" (תכף אסביר) שעבור ערכים מרוכבים {% equation %}s=a+bi{% endequation %} שעבורם {% equation %}a&gt;1{% endequation %} מקבלת את אותו הערך כמו {% equation %}\sum\frac{1}{n^{s}}{% endequation %} (לא קשה להראות כי לכל {% equation %}s{% endequation %} מרוכב שכזה הטור אכן מתכנס).

התכונה ה"נחמדה" שאני מדבר עליה נקראת <strong><a href="http://he.wikipedia.org/wiki/%D7%A4%D7%95%D7%A0%D7%A7%D7%A6%D7%99%D7%94_%D7%90%D7%A0%D7%9C%D7%99%D7%98%D7%99%D7%AA">אנליטיות</a></strong>. פונקציה מרוכבת היא אנליטית בתחום מסויים ("תחום" כאן הוא במשמעות של "קבוצה פתוחה" - זה לא קריטי להבנת המושג) אם היא גזירה בכל נקודה של התחום. אם ההגדרה הזו לא אומרת לכם הרבה, לא נורא - מה שחשוב כאן הוא שהאובייקט המרכזי שבו מתעניינים בתורת הפונקציות המרוכבות הוא פונקציות אנליטיות שכאלו, שכן האנליטיות שלהן מבטיחה שיהיו להן תכונות מאוד, מאוד יפות; ומצד שני, מרבית הפונקציות ה"רגילות" שאנחנו מכירים (פולינומים, אקספוננט, לוגריתם, סינוסים וקוסינוסים...) הן אכן אנליטיות. מה שרימן עשה הוא מקרה פרטי של דבר מה כללי יותר - פונקציות רבות ניתנות ל"המשכה אנליטית" - הגדרה שלהן על תחום גדול יותר, באופן שהפונקציה המתקבלת תהיה אנליטית, וזהה לפונקציה המקורית על התחום שבו שתיהן מוגדרות. לקרוא לפונקציה שהתקבלה "אנליטית" זה טיפה שגוי שכן בנקודה {% equation %}s=1{% endequation %} גם הפונקציה המורחבת נתקלת בבעיה ו"מתפוצצת" (שואפת לאינסוף; פורמלית, לנקודה כזו קוראים <strong>קוטב</strong>); השם המדוייק לפונקציה שכזו היא "מרומורפית", אבל לא ניכנס לזה.

מה שאני בעיקר רוצה להבהיר כאן, מעבר ל"{% equation %}\zeta\left(s\right){% endequation %} היא פונקציה מרוכבת יפה", הוא שלא נכון לחשוב על {% equation %}\zeta\left(s\right){% endequation %} רק בתור הטור {% equation %}\sum\frac{1}{n^{s}}{% endequation %}; זה בלבול נפוץ שגם אני לקיתי בו. עבור ערכי {% equation %}s{% endequation %} מרוכב שהחלק הממשי שלהם אינו גדול מ-1, הפונקציה מוגדרת באופן אחר, מסובך יותר; אין לנו נוסחה פשוטה ויפה עבורה (ואם הייתה כזו, הרבה בעיות היו נפתרות...). מה שכן יש, וגם את זה רימן הוכיח במאמרו, הוא מה שנקרא <strong>משוואה פונקציונלית</strong> - משוואה שבה {% equation %}\zeta\left(s\right){% endequation %} מופיעה יחד עם פונקציות נוספות ומאפשרת ללמוד רבות על ההתנהגות של {% equation %}\zeta\left(s\right){% endequation %}.

בצורה הידידותית ביותר שאני מכיר, אפשר לכתוב את המשוואה הפונקציונלית בתור {% equation %}\zeta\left(1-s\right)=\gamma\left(s\right)\zeta\left(s\right){% endequation %}, כאשר הפונקציה {% equation %}\gamma\left(s\right){% endequation %} היא פונקציה מסובכת יחסית; לצורך שלמות אכתוב אותה במפורש: {% equation %}\gamma\left(s\right)=\pi^{\frac{1}{2}-s}\Gamma\left(\frac{s}{2}\right)/\Gamma\left(\frac{1-s}{2}\right){% endequation %}. גם כאן הסיפור לא נגמר כי לא הסברתי מהי {% equation %}\Gamma{% endequation %}; זוהי פונקציה חשובה בפני עצמה באנליזה מרוכבת המכונה בפשטות "<a href="http://he.wikipedia.org/wiki/%D7%A4%D7%95%D7%A0%D7%A7%D7%A6%D7%99%D7%99%D7%AA_%D7%92%D7%90%D7%9E%D7%94">פונקציית גאמה</a>" ({% equation %}\Gamma{% endequation %} היא האות היוונית גאמה). לא אכנס כאן לתכונות שלה יותר מדי; רק אציין שהיא מהווה הרחבה של פונקצית העצרת (שבתורה מוגדרת רק למספרים טבעיים) לכלל המספרים המרוכבים; מתקיים {% equation %}\Gamma\left(n\right)=\left(n-1\right)!{% endequation %}.

מה אנחנו יכולים ללמוד מהמשוואה הפונקציונלית? בראש ובראשונה, היכן נמצאים האפסים של {% equation %}\zeta\left(s\right){% endequation %}. נתחיל מכך שידוע כי פונקצית הגאמה "מתנהגת יפה" לכל ערך פרט לערכים מהצורה {% equation %}-n{% endequation %}, כלומר מספרים שלמים שליליים ({% equation %}-1,-2,-3,\dots{% endequation %}). על ערכים כאלו הפונקציה "מתפוצצת" - שואפת לאינסוף. מכאן ש-{% equation %}\gamma\left(s\right){% endequation %} סובלת מ"התפוצצות" דומה עבור ערכים מהצורה {% equation %}s=-2n{% endequation %} - כלומר, מספרים שלמים שליליים <strong>זוגיים</strong> (הסיבה לכך היא ש-{% equation %}\gamma{% endequation %} מתפוצצת כאשר הרכיב של {% equation %}\Gamma\left(\frac{s}{2}\right){% endequation %} שבתוכה מתפוצץ, כלומר כאשר {% equation %}\frac{s}{2}{% endequation %} הוא שלם שלילי, ולכן בהכרח {% equation %}s{% endequation %} הוא שלם שלילי זוגי). מכיוון שעבור {% equation %}s=-2n{% endequation %} מתקיים {% equation %}\zeta\left(1-s\right)=\zeta\left(1+2n\right){% endequation %} ו-{% equation %}\zeta\left(1+2n\right){% endequation %} הוא מספר סופי בעליל (אמרנו שהטור שמגדיר את {% equation %}\zeta{% endequation %} עבור {% equation %}s{% endequation %}-ים שהחלק הממשי שלהם הוא גדול מ-1 מתכנס בכל התחום הזה), בהכרח גם {% equation %}\gamma\left(s\right)\zeta\left(s\right){% endequation %} חייב להיות סופי בעליל, ולכן {% equation %}\zeta\left(s\right){% endequation %} חייב להתאפס ב-{% equation %}s{% endequation %}-ים הללו (אחרת שום דבר לא יאזן את ה"התפוצצות" של {% equation %}\gamma{% endequation %}). מסקנה: ל-{% equation %}\zeta\left(s\right){% endequation %} יש אפסים עבור כל המספרים השלמים השליליים הזוגיים. האפסים הללו מכונים "האפסים הטריוויאליים של פונקצית הזטא". מה שמעניין אותנו הוא מיהם האפסים הנותרים.

רגע, רגע - עוד לא הסברתי <strong>למה</strong> בעצם האפסים הנותרים כל כך מעניינים אותנו. גם אין לי שום דרך להסביר בצורה משכנעת מבלי להיכנס לפרטים טכניים שהם יותר מדי בשביל הפוסט הזה, אז הנה נפנוף ידיים גמור - באופן כללי ניתן ללמוד הרבה על התנהגות של פונקציה מהאפסים שלה, ולהשיג תיאור יחסית טוב שלה באמצעות האפסים. הדוגמה הפשוטה ביותר לכך היא <a href="http://he.wikipedia.org/wiki/%D7%A4%D7%95%D7%9C%D7%99%D7%A0%D7%95%D7%9D">פולינום</a>; פולינום ניתן לכתיבה בתור מכפלה מהצורה {% equation %}\prod\left(x-a_{i}\right){% endequation %} כאשר {% equation %}a_{i}{% endequation %} הם האפסים שלו. לפונקציות מרוכבות אנליטיות כלליות יש תיאור דומה כמכפלה, אם כי מסובך יותר (<a href="http://en.wikipedia.org/wiki/Weierstrass_factorization_theorem">מכפלת ויירשטראס</a>). אין מנוס - תצטרכו להאמין לי שזהו המצב, ושזוהי הסיבה שבגללה האפסים מעניינים. הבנה מהם האפסים של הפונקציה - בפרט, איפה הם נמצאים - היא המפתח להבנה של התנהגות הפונקציה.

אוקיי, אז איפה עוד יש אפסים? אם החלק הממשי של {% equation %}s{% endequation %} גדול מ-1 אז {% equation %}\zeta\left(s\right){% endequation %} שונה מאפס - פשוט בגלל שבתחום הזה הפונקציה מוגדרת באמצעות הטור {% equation %}\sum\frac{1}{n^{s}}{% endequation %} והוא מתכנס למספר שונה מאפס (אם {% equation %}s{% endequation %} ממשי זה מיידי לראות זאת; אחרת אפשר לקחת את הטור הזה בערך מוחלט). עכשיו, אם {% equation %}s{% endequation %} הוא שורש של {% equation %}\zeta\left(s\right){% endequation %} אז מהמשוואה הפונקציונלית נובע שגם {% equation %}1-s{% endequation %} הוא שורש של {% equation %}\zeta{% endequation %}, אלא אם {% equation %}s=-2n{% endequation %}, כלומר אם הוא אחד מהאפסים הטריוויאליים הידועים לשמצה. מדוע? כי אם {% equation %}\zeta\left(s\right){% endequation %} מתאפס ו-{% equation %}s\ne-2n{% endequation %} אז {% equation %}\gamma\left(s\right){% endequation %} מתנהג "נחמד" ומכאן ש-{% equation %}\gamma\left(s\right)\zeta\left(s\right)=0{% endequation %}, ולכן {% equation %}\zeta\left(1-s\right)=\gamma\left(s\right)\zeta\left(s\right)=0{% endequation %} . מכאן נובע מייד שאם החלק הממשי של {% equation %}s{% endequation %} קטן מאפס והוא אינו מהצורה {% equation %}-2n{% endequation %}, הוא אינו יכול להיות שורש של {% equation %}\zeta{% endequation %} אחרת גם {% equation %}1-s{% endequation %} היה שורש, אבל {% equation %}1-s{% endequation %} הוא במקרה זה מספר שהחלק הממשי שלו גדול מ-1, וראינו שהם לא שורשים. מסקנה: האפסים של פונקצית הזטה חייבים להיות כאלו שערכם הממשי הוא בין 0 ל-1, והם מפוזרים באופן סימטרי; אם {% equation %}s{% endequation %} שורש, גם {% equation %}1-s{% endequation %} הוא שורש. קל לראות שציר הסימטריה כאן הוא הישר האנכי שעובר דרך {% equation %}\frac{1}{2}{% endequation %} (יותר במדוייק, ערכי ה-{% equation %}x{% endequation %} משוקפים דרך הישר {% equation %}x=\frac{1}{2}{% endequation %}; ערכי ה-{% equation %}y{% endequation %} משוקפים גם הם, דרך {% equation %}y=0{% endequation %}).

בקיצור, אפשר לחשוב על כל האפסים הלא טריוויאליים כאילו הם נמצאים בתוך "רצועה" (שטח אינסופי התחום בין שני קווים אופקיים) שנמתחת באופן סימטרי סביב הישר האנכי שעובר ב-{% equation %}\frac{1}{2}{% endequation %}. הרצועה הזו מכונה "הרצועה הקריטית", והשאלה שנשאלת היא מה רוחב הרצועה הזו; והשערת רימן אומרת שהרוחב הוא אפס, כלומר שכל השורשים של פונקציית הזטא נמצאים <strong>על</strong> הקו האנכי {% equation %}Re\left(z\right)=\frac{1}{2}{% endequation %}.

רימן עצמו מעלה את ההשערה הזו כמעט בדרך אגב ב<a href="http://www.claymath.org/millennium/Riemann_Hypothesis/1859_manuscript/">מאמרו</a>. בעמוד 4 שלו הוא עוסק בפונקציה שהוא מכנה {% equation %}\xi\left(t\right){% endequation %} ושמוגדרת באופן מסויים באמצעות פונקצית הזטא של רימן; הוא מגדיר אותה באמצעות שינוי המשתנה {% equation %}s=\frac{1}{2}+it{% endequation %}, כלומר {% equation %}t{% endequation %} מתאר "הזזה" של {% equation %}s{% endequation %} מהנקודה {% equation %}\frac{1}{2}{% endequation %}. אם {% equation %}t{% endequation %} ממשי אז ההזה היא רק מעלה ומטה, כלומר ה-{% equation %}s{% endequation %} המתאים הוא עדיין בעל חלק ממשי {% equation %}\frac{1}{2}{% endequation %}; ואפס של {% equation %}\xi{% endequation %} הוא אפס של {% equation %}\zeta{% endequation %}, כך שאפשר לנסח את השערת רימן בתור "השורשים של {% equation %}\xi{% endequation %} הם ממשיים", וזה בדיוק מה שרימן עושה (בתרגום חופשי שלי של התרגום לאנגלית):

"...זה מאוד סביר כי כל השורשים הם ממשיים. בהחלט ניתן לקוות כאן להוכחה מדוייקת יותר; לאחר מספר נסיונות כושלים הפסקתי לעת עתה את החיפוש אחר הוכחה כזו מאחר שאין לי צורך בה עבור היעד הבא שלי". ואכן, לרימן לא היה בה צורך - ואכן, בהחלט ניתן לקוות להוכחה מדוייקת יותר.

הבה ונחזור כעת לראשוניים. רימן ניסה להעריך את הפונקציה {% equation %}\pi\left(x\right){% endequation %}, שלכל {% equation %}x{% endequation %} טבעי מחזירה את מספר הראשוניים הקטנים ממנו. הראשונים שהעלו השערות (אמפיריות, על פי תוצאות "ניסויים" שהם ערכו) לגבי התנהגות הפונקציה הזו היו גאוס ולז'נדר, בנפרד. שניהם ניסו לתאר את הפונקציה הזו באמצעות פונקציה פשוטה יחסית; הפונקציה שהם מצאו הייתה {% equation %}\frac{x}{\ln x}{% endequation %} עבור לג'נדר, ופונקציה קצת יותר מורכבת שנקראת {% equation %}li{% endequation %} שעליה חשב גאוס. המשמעות המדוייקת של הקירובים של לג'נדר וגאוס היא שהיחס בין {% equation %}\pi\left(x\right){% endequation %} והפונקציות שלהם שואף ל-1 כאשר {% equation %}x{% endequation %} שואף לאינסוף; אלא שהם לא הוכיחו זאת. ההוכחה חיכתה בסבלנות מאה שנים, ורק לקראת סוף המאה ה-19 הוכיחו שני מתמטיקאים - פוסן והדמר, כל אחד בנפרד - את התוצאה הזו שמכונה "<a href="http://he.wikipedia.org/wiki/%D7%9E%D7%A9%D7%A4%D7%98_%D7%94%D7%9E%D7%A1%D7%A4%D7%A8%D7%99%D7%9D_%D7%94%D7%A8%D7%90%D7%A9%D7%95%D7%A0%D7%99%D7%99%D7%9D">משפט המספרים הראשוניים</a>". ההוכחות שלהם עשו שימוש מהותי בפונקצית הזטא של רימן וברעיונות שרימן העלה באותו מאמר שלו, כמעט ארבעים שנה לפני כן; בפרט, צעד קריטי בהוכחה היה להראות כי לפונקצית הזטא אין שורשים עם ערך ממשי 1, כלומר אין לה אפסים על "קצוות" הרצועה הקריטית.

אם כן, האם זה סוף הסיפור? ממש לא, שכן הקירוב של משפט המספרים הראשוניים הוא קצת בעייתי. אפשר לדבר על שני סוגי קירובים - חיבורי, וכפלי. קירוב כפלי זה משהו בסגנון "הערך שאני מציע הוא לא יותר מפי 2 מהערך האמיתי"; קירוב חיבורי הוא משהו בסגנון "הערך שאני מציע לא גדול מהערך האמיתי ביותר מ-30". אני מניח שההבדל הרעיוני ברור. אם יש לי פונקציה {% equation %}f\left(x\right){% endequation %}, אז {% equation %}g\left(x\right){% endequation %} היא קירוב כפלי אם {% equation %}\frac{f\left(x\right)}{g\left(x\right)}\le k\left(x\right){% endequation %} עבור {% equation %}k\left(x\right){% endequation %} כלשהו, לכל {% equation %}x{% endequation %} החל ממקום מסויים; היא קירוב חיבורי אם {% equation %}\left|f\left(x\right)-g\left(x\right)\right|\le k\left(x\right){% endequation %} עבור {% equation %}k\left(x\right){% endequation %} כלשהו, לכל {% equation %}x{% endequation %} החל ממקום מסויים. משפט המספרים הראשוניים נותן קירוב כפלי, לא חיבורי.

בואו נמחיש את ההבדל בין שני סוגי הקירובים. נתבונן בפונקציה {% equation %}f\left(x\right)=2^{x}+p\left(x\right){% endequation %}. כאשר {% equation %}p\left(x\right){% endequation %} הוא פולינום כלשהו. ניקח לפונקציה הזו בתור קירוב את {% equation %}g\left(x\right)=2^{x}{% endequation %}. אז לא קשה לראות כי {% equation %}\frac{f\left(x\right)}{g\left(x\right)}\to1{% endequation %} כאשר {% equation %}x{% endequation %} שואף לאינסוף, שכן {% equation %}\frac{p\left(x\right)}{g\left(x\right)}\to0{% endequation %} (פולינום תמיד גדל מהותית לאט יותר מכל פונקציה אקספוננציאלית). לעומת זאת, {% equation %}\left|f\left(x\right)-g\left(x\right)\right|=p\left(x\right){% endequation %}, כלומר הקירוב החיבורי יכול להיות עם שגיאה גדולה מאוד במקרה הזה, אפילו {% equation %}p\left(x\right)=x^{100^{100}}{% endequation %}. כלומר, קירוב כפלי טוב הוא עדיין קירוב גס יחסית; כדי לשפר את משפט המספרים הראשוניים, רוצים למצוא קירוב <strong>חיבורי</strong> טוב.

לצורך קירוב חיבורי, {% equation %}\frac{x}{\ln x}{% endequation %} של לג'נדר כבר אינה כל כך מוצלחת, ולכן מתמקדים בפונקציה שעליה גאוס חשב, {% equation %}li{% endequation %} (מלשון Logarithmic Integral) המוגדרת בתור {% equation %}li\left(x\right)=\int_{2}^{x}\frac{dt}{\ln t}{% endequation %}. זוהי פונקציה שדומה ל-{% equation %}\frac{x}{\ln x}{% endequation %} ועוד "תיקונים"; לא אכנס כאן לפרטים. כל עוד מדברים על משפט המספרים הראשוניים, אין הבדל בין {% equation %}\frac{x}{\ln x}{% endequation %} ובין {% equation %}li\left(x\right){% endequation %}; מתקיים גם {% equation %}\frac{\pi\left(x\right)}{li\left(x\right)}\to1{% endequation %} ולכן זהו אותו משפט עבור שתיהן. עבור קירוב חיבורי העניינים שונים.

אינטואיציה כלשהי לגבי שתי הפונקציות ומדוע {% equation %}li{% endequation %} טובה יותר ניתן לתת באמצעות נפנוף הידיים הבא: בכל איזור נתון של המספרים הטבעיים, ההסתברות שיצוץ מספר ראשוני היא בערך {% equation %}\frac{1}{\ln n}{% endequation %}, כש-{% equation %}n{% endequation %} הוא המספר הנוכחי שאנחנו בודקים. הקירוב {% equation %}\pi\left(x\right)\approx\frac{x}{\ln x}{% endequation %} הוא קירוב "גס" במובן זה שהוא אומר "בואו ניקח את קצה התחום שלנו, {% equation %}x{% endequation %}, ונניח שההסתברות <strong>עבור כל מספר בתחום</strong> שהוא ראשוני שווה להסתברות ש-{% equation %}x{% endequation %} עצמו ראשוני, כלומר {% equation %}\frac{1}{\ln x}{% endequation %}, ונחשב את מספר הראשוניים על בסיס ההנחה הזו". זו גישה מעט גסה, אבל יש מקרים שבהם היא עובדת מצויין: למשל, אם ניקח מספר טבעי אקראי (אני מנפנף בידיים אז לא ניכנס לשאלה מה זה בכלל אומר, "מספר טבעי אקראי"), ההסתברות שהוא זוגי היא {% equation %}\frac{1}{2}{% endequation %}, ואכן - {% equation %}\frac{x}{2}{% endequation %} היא הערכה טובה ל"כמות המספרים הטבעיים הזוגיים עד {% equation %}x{% endequation %}". הבעיה עם {% equation %}\frac{x}{\ln x}{% endequation %} היא שכאן ההסתברות משתנה (וקטנה) עם הזמן, כך שלהחיל את ההערכה של {% equation %}\frac{1}{\ln x}{% endequation %} על כל המספרים גורם לנו לאבד משהו - אלא שכמו שמשפט המספרים הראשוניים הראה, לא מאבדים <strong>הרבה</strong>.

לעומת זאת, {% equation %}li\left(x\right){% endequation %} מתחשב הרבה יותר במה שקורה באמצע - זו המשמעות של האינטגרל. הוא מבצע מעין "מיצוע" של ההסתברות לקבלת ראשוני על פני כל הישר הממשי עד {% equation %}x{% endequation %} (ה-2 בהתחלה מטרתו למנוע שאלות של "מתי האינטגרל מתכנס" והוא לא משנה באופן מהותי את הערך של הפונקציה). מה שחשוב להדגיש כאן הוא שגם {% equation %}\frac{x}{\ln x}{% endequation %} וגם {% equation %}li\left(x\right){% endequation %} שתיהן פונקציות <strong>פשוטות</strong> יחסית - פשוטות להבנה ולניתוח, וגם פשוטות לחישוב, וזאת להבדיל מ-{% equation %}\pi\left(x\right){% endequation %} הבלתי מושגת.

וכעת אנו מגיעים סוף סוף לקשר שבין השערת רימן וכל זה. בערך עשרים שנים לאחר הוכחת משפט המספרים הראשוניים הוכיח הלגה פון-קוך (אותו אחד מ"פתית השלג של קוך") שהשערת רימן <strong>שקולה</strong> לטענה ש-{% equation %}li{% endequation %} מהווה קירוב חיבורי ל-{% equation %}\pi\left(x\right){% endequation %} עם גודל שגיאה שחסום על ידי {% equation %}\sqrt{x}\ln x{% endequation %} - גודל שקטן משמעותית מהגודל של {% equation %}li{% endequation %}, מה שמצביע על כך ש-{% equation %}li{% endequation %} היא קירוב מאוד, מאוד טוב. במובן מסויים (שלא אכנס אליו כרגע) יש קשר הדוק בין <strong>רוחב</strong> הרצועה הקריטית ואיכות הקירוב של {% equation %}\pi\left(x\right){% endequation %} - ככל שהרצועה הקריטית יותר צרה, הקירוב טוב יותר (ולהפך - קירוב טוב יותר גורר שהרצועה הקריטית צרה יותר), כשהמצב האופטימלי מושג כשהרצועה הופכת להיות הקו האנכי בלבד. כלומר, השערת רימן, בניסוחה השקול, אומרת שהקירוב הטוב ביותר של {% equation %}\pi\left(x\right){% endequation %} שאליו אנחנו יכולים לקוות (עם פונקציות סטייל {% equation %}li\left(x\right){% endequation %}) הוא אכן מה שאנו משיגים בפועל.

איך אומרים את כל זה באופן מדוייק? איך ומהיכן מגיע הקשר הזה שבין הקירוב של {% equation %}\pi\left(x\right){% endequation %} ובין האפסים של פונקצית הזטא? אלו שאלות קשות, והתשובה עליהן היא טכנית ומורכבת. ייתכן שארחיב על כך בפוסטים נוספים בעתיד.

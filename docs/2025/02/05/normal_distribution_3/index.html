<!DOCTYPE html>
<html lang="he" dir="rtl">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>מה הקטע עם התפלגות נורמלית? (חלק ג&#39;: על תוחלת וסטיית תקן) - לא מדויק</title>
    <link rel="icon" type="image/x-icon" href="/new_site/favicon.ico">
    
    
    <!-- KaTeX for math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>

    
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Arial', 'David', 'Tahoma', sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
        }
        
        /* Top navigation bar */
        .top-nav {
            background: #2c3e50;
            padding: 15px 20px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
        
        .top-nav .nav-container {
            max-width: 1200px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            justify-content: space-between;
            gap: 20px;
        }
        
        .top-nav .site-title {
            font-size: 1.5em;
            font-weight: bold;
            color: white;
            text-decoration: none;
            white-space: nowrap;
        }
        
        .top-nav .nav-links {
            display: flex;
            gap: 20px;
            align-items: center;
            flex-wrap: wrap;
        }
        
        .top-nav .nav-links a {
            color: #ecf0f1;
            text-decoration: none;
            padding: 5px 10px;
            border-radius: 4px;
            transition: background 0.2s;
            white-space: nowrap;
        }
        
        .top-nav .nav-links a:hover {
            background: #34495e;
        }
        
        .top-nav .search-box {
            display: flex;
            gap: 5px;
        }
        
        .top-nav .search-box input {
            padding: 5px 10px;
            border: none;
            border-radius: 4px;
            font-size: 0.9em;
            min-width: 150px;
        }
        
        .top-nav .search-box button {
            padding: 5px 15px;
            background: #3498db;
            color: white;
            border: none;
            border-radius: 4px;
            cursor: pointer;
            transition: background 0.2s;
        }
        
        .top-nav .search-box button:hover {
            background: #2980b9;
        }
        
        /* Blockquote styling */
        blockquote {
            background: #f9f9f9;
            border-right: 5px solid #ccc;
            margin: 1.5em 10px;
            padding: 0.75em 16px;
            position: relative;
            font-style: italic;
        }
        
        blockquote:before {
            content: '"';
            position: absolute;
            top: -10px;
            right: 8px;
            font-size: 3.5em;
            color: #ccc;
            line-height: 1;
        }
        
        blockquote p {
            display: block;
            margin: 0 0 0.75em 0;
        }
        
        blockquote p:last-child {
            margin-bottom: 0;
        }
        
        
        body {
            line-height: 1.8;
        }
        
        .container {
            max-width: 800px;
            margin: 20px auto;
            background: white;
            padding: 40px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            border-radius: 8px;
        }
        
        header {
            margin-bottom: 40px;
            border-bottom: 2px solid #e0e0e0;
            padding-bottom: 20px;
        }
        
        h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
            color: #2c3e50;
        }
        
        .post-meta {
            color: #7f8c8d;
            font-size: 0.9em;
        }
        
        .post-meta .date {
            margin-left: 15px;
        }
        
        .post-meta .categories,
        .post-meta .tags {
            display: inline;
        }
        
        .post-meta .categories a,
        .post-meta .tags a {
            color: #3498db;
            text-decoration: none;
            margin: 0 5px;
        }
        
        .post-meta .categories a:hover,
        .post-meta .tags a:hover {
            text-decoration: underline;
        }
        
        article {
            font-size: 1.1em;
        }
        
        article h2 {
            margin-top: 30px;
            margin-bottom: 15px;
            color: #34495e;
            font-size: 1.8em;
        }
        
        article h3 {
            margin-top: 25px;
            margin-bottom: 12px;
            color: #34495e;
            font-size: 1.4em;
        }
        
        article p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        article ul, article ol {
            margin-right: 30px;
            margin-bottom: 15px;
        }
        
        article li {
            margin-bottom: 8px;
        }
        
        article code {
            background-color: #f4f4f4;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
        }
        
        article pre {
            background-color: #2d2d2d;
            color: #f8f8f2;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
            margin-bottom: 20px;
            direction: ltr;
            text-align: left;
        }
        
        article pre code {
            background-color: transparent;
            padding: 0;
            color: inherit;
        }
        
        /* Image styles - responsive and contained */
        article img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
        }
        
        /* Math rendering styles */
        .math {
            direction: ltr;
        }
        
        span.math {
            direction: ltr;
        }
        
        /* Override RTL for KaTeX */
        .katex {
            direction: ltr;
            unicode-bidi: embed;
        }
        
        div.math {
            display: block;
            text-align: center;
            padding: 15px 0;
            direction: ltr;
        }
        
        /* RTL adjustments for code blocks */
        .highlight {
            direction: ltr;
            text-align: left;
        }
        
        footer {
            margin-top: 60px;
            padding-top: 20px;
            border-top: 1px solid #e0e0e0;
            text-align: center;
            color: #95a5a6;
            font-size: 0.9em;
        }
        
        /* Responsive design */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }
            
            h1 {
                font-size: 2em;
            }
            
            article {
                font-size: 1em;
            }
        }
        
        /* Post navigation */
        .post-navigation {
            display: flex;
            justify-content: space-between;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 2px solid #e0e0e0;
            gap: 20px;
        }
        
        .post-navigation .nav-link {
            flex: 1;
            text-decoration: none;
            color: #2c3e50;
            padding: 15px;
            background: #f8f9fa;
            border-radius: 5px;
            transition: background 0.2s;
        }
        
        .post-navigation .nav-link:hover {
            background: #e9ecef;
        }
        
        .post-navigation .nav-prev {
            text-align: right;
        }
        
        .post-navigation .nav-next {
            text-align: left;
        }
        
        .post-navigation .nav-label {
            font-size: 0.85em;
            color: #7f8c8d;
            display: block;
            margin-bottom: 5px;
        }
        
        .post-navigation .nav-title {
            font-weight: bold;
            font-size: 1.1em;
        }

    </style>
</head>
<body>
    <nav class="top-nav">
        <div class="nav-container">
            <a href="/new_site/" class="site-title">לא מדויק</a>
            <div class="nav-links">
                <a href="/new_site/">דף הבית</a>
                <a href="/new_site/random.html">פוסט אקראי</a>
                <a href="/new_site/post_list.html">כל הפוסטים</a>
                <div class="search-box">
                    <input type="text" id="searchInput" placeholder="חיפוש...">
                    <button onclick="searchPosts()">חפש</button>
                </div>
            </div>
        </div>
    </nav>
    
    
    <div class="container">
        <!-- Post Navigation -->
        
        <nav class="post-navigation">
            
            <a href="/new_site/2025/02/03/normal_distribution_2/" class="nav-link nav-prev">
                <span class="nav-label">פוסט ישן יותר →</span>
                <span class="nav-title">מה הקטע עם התפלגות נורמלית? (חלק ב&#39;: על שתי הסתברויות)</span>
            </a>
            

            
            <a href="/new_site/2025/02/07/normal_distribution_4/" class="nav-link nav-next">
                <span class="nav-label">← פוסט חדש יותר</span>
                <span class="nav-title">מה הקטע עם התפלגות נורמלית? (חלק ד&#39; ואחרון: משפט הגבול המרכזי)</span>
            </a>
            
        </nav>
        
        
        <header>
            <h1>מה הקטע עם התפלגות נורמלית? (חלק ג&#39;: על תוחלת וסטיית תקן)</h1>
            <div class="post-meta">
                <span class="date">2025-02-05</span>
                
                <span class="categories">
                    | קטגוריות:
                    
                    <a href="/categories/הסתברות.html">הסתברות</a>
                    
                </span>
                
                
                <span class="tags">
                    | תגיות:
                    
                    <a href="/tags/התפלגות נורמלית.html">התפלגות נורמלית</a>
                    
                </span>
                
            </div>
        </header>
        
        <article>
            <h2>מבוא</h2>

<p>בסדרת הפוסטים הנוכחית אנחנו מנסים להבין למה ההתפלגות הנורמלית <span class="math">\(f\left(x\right)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\left(x-\mu\right)^{2}/2\sigma^{2}}\)</span> נראית כמו שהיא נראית. <a href="https://gadial.net/2025/02/03/normal_distribution_2/">בפוסט הקודם</a> הבנו מה הפונקציה הזו בכלל אומרת - זו <strong>פונקציית צפיפות ההסתברות</strong> של ההתפלגות הנורמלית, כלומר אם <span class="math">\(X\)</span> הוא המשתנה המקרי שמתואר על ידי הפונקציה הזו, אז <span class="math">\(P\left(a\le X\le b\right)=\int_{a}^{b}\frac{1}{\sqrt{2\pi}\sigma}e^{-\left(x-\mu\right)^{2}/2\sigma^{2}}dx\)</span>. בפוסט הקודם גם ראינו מאיפה ה-<span class="math">\(\pi\)</span> הגיע - זה קבוע נרמול שנדרש כדי שיתקיים <span class="math">\(\int_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{x^{2}}{2}}=1\)</span>. אבל מה הם ה-<span class="math">\(\mu\)</span> וה-<span class="math">\(\sigma\)</span> שמופיעים בנוסחה? אלו שני פרמטרים שמאפיינים לא רק את ההתפלגות הזו, אלא כל התפלגות שהיא: <strong>התוחלת</strong> <span class="math">\(\mu\)</span> של ההתפלגות, <strong>וסטיית התקן</strong> <span class="math">\(\sigma\)</span> שלה. באופן לא פורמלי, תוחלת של התפלגות היא הממוצע שלה, וסטיית התקן מלמדת אותנו כמה גדול הפיזור של ההתפלגות סביב הממוצע הזה. מה שמפליא הוא כמה מידע על ההתפלגות נמצא כבר בשני הערכים המספריים הללו - וההמחשה לזה היא בדיוק <strong>משפט הגבול המרכזי</strong>, שאומר שאם אנחנו יודעים את התוחלת וסטיית התקן של התפלגות, אז נדע בדיוק איך נראה מה שמתקבל מסכומים גדולים של משתנים מקריים בלתי תלויים בעלי ההתפלגות הזו.</p>
<p>בואו נעבור להגדרות פורמליות, ונראה איך זה בא לידי ביטוי בהתפלגות נורמלית ובהתפלגויות אחרות שראינו.</p>
<h2>תוחלת</h2>

<p>מה זה "ממוצע"? אם למשל יש לנו את סדרת המספרים <span class="math">\(10,30,50,60,90\)</span> הממוצע שלה היא הסכום של כל המספרים חלקי כמה מספרים יש: <span class="math">\(\frac{10+30+50+60+90}{5}=\frac{240}{5}=48\)</span>. מה המשמעות של המספר הזה, 48? אפשר לחשוב על זה כך: אם נחליף כל איבר בסדרה ב-48, נקבל סדרה <span class="math">\(48,48,48,48,48\)</span> שסכום האיברים שלה שווה לסכום אברי הסדרה המקורית.</p>
<p>את הרעיון הזה אפשר להכליל קצת: אם למשל הסדרה שלנו היא <span class="math">\(30,60,60\)</span> אז הממוצע שלה הוא <span class="math">\(50\)</span>, אבל אפשר גם "לאחד" את שני ה-60-ים ולהגיד שיש בסדרה שלנו שני איברים: האיבר <span class="math">\(30\)</span> עם <strong>המשקל</strong> 1, והאיבר <span class="math">\(60\)</span> עם <strong>המשקל</strong> 2. מה שאנחנו עושים הוא לסכום את האיברים כשכל אחד מוכפל במשקל שלו, ולחלק בסכום הכולל של המשקלים. אולי קל יותר להבין את זה עם נוסחה: אם יש לנו את הסדרה <span class="math">\(a_{1},a_{2},\ldots,a_{n}\)</span> ואת המשקלים <span class="math">\(w_{1},\ldots,w_{n}\)</span> אז <strong>הממוצע המשוקלל</strong> הוא <span class="math">\(\frac{\sum_{i=1}^{n}w_{i}a_{i}}{\sum_{i=1}^{n}w_{i}}\)</span>. כאשר כל המשקלים הם 1 אנחנו מקבלים את הגדרת הממוצע הקודמת.</p>
<p>ההגדרה של ממוצע משוקלל לא דורשת שום דבר מהמשקלים, חוץ מכך שהסכום שלהם יהיה שונה מאפס (אחרת יקרה משהו מוזר כשמנסים לחלק) אבל כמובן, ההגדרה הזו יוצאת פשוטה במיוחד אם <span class="math">\(0\le w_{i}\le1\)</span> ובנוסף <span class="math">\(\sum_{i=1}^{n}w_{i}\)</span>; אנחנו מקבלים שהממוצע המשוקלל הוא פשוט <span class="math">\(\sum_{i=1}^{n}w_{i}a_{i}\)</span>.</p>
<p>איך כל זה קשור להסתברות? ובכן, הסתברות היא בדיוק המקרה שבו יש לנו איברים <span class="math">\(0\le w_{i}\le1\)</span> שמסתכמים ל-1: אם יש לנו מרחב הסתברות ו-<span class="math">\(X\)</span> הוא משתנה מקרי שמקבל מספר <strong>סופי</strong> של ערכים, נאמר הערכים <span class="math">\(a_{1},a_{2},\ldots,a_{n}\)</span>, אז אפשר להסתכל על הממוצע המשוקלל <span class="math">\(\sum_{i=1}^{n}P\left(X=a_{i}\right)\cdot a_{i}\)</span>. כלומר, אנחנו לוקחים את הממוצע המשוקלל של ה-<span class="math">\(a_{i}\)</span>-ים כשהמשקולות הן בדיוק ההסתברויות ש-<span class="math">\(a_{i}\)</span> יעלה בגורל. סכום כזה נקרא <strong>תוחלת</strong> ומסומן ב-<span class="math">\(E\left[X\right]\)</span>. אם מרחב המדגם הוא אינסופי העניינים מן הסתם מסתבכים קצת מתמטית אבל הרעיון זהה: <span class="math">\(\text{E}\left[X\right]=\sum_{i=1}^{\infty}P\left(X=a_{i}\right)\cdot a_{i}\)</span> עבור מרחב מדגם אינסופי בדיד, ו-<span class="math">\(\text{E}\left[X\right]=\int_{-\infty}^{\infty}p\left(x\right)xdx\)</span> עבור מרחב מדגם אינסופי רציף.</p>
<p>למה ממוצע משוקלל כזה הוא מעניין? אנחנו הרי מצפים שהוא ילמד אותנו משהו על המשתנה המקרי <span class="math">\(X\)</span> ולא סתם יהיה הגדרה לשם הגדרה. כאן זו אחת מהסיטואציות הנדירות שבהן המתמטיקה הייתה מדע <strong>אמפירי</strong>: קודם כל התגלתה תופעה, ואחר כך הוכיחו אותה מתמטית. התופעה הייתה האבחנה שאם אנחנו חוזרים על אותו ניסוי הסתברותי מספרי שוב ושוב, <strong>הממוצע</strong> של התוצאות שלנו נוטה להיות יציב יחסית ככל שמספר הניסויים שאנחנו מבצעים גדל. בואו נראה דוגמא קונקרטית לזה עם הטלת קוביה. בימינו, למרבה השמחה, אפשר לרתום את המחשב לניסויים כאלו, אז הנה קוד פייתון קצרצר: </p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">random</span>
<span class="k">for</span> <span class="n">N</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">10000</span><span class="p">,</span> <span class="mi">100000</span><span class="p">,</span> <span class="mi">1000000</span><span class="p">]:</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">N</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="nb">sum</span><span class="p">([</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">6</span><span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">_</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">)])</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">N</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<p>מה שהקוד הזה עושה הוא להטיל קוביה מספר כלשהו של פעמים שמסומן ב-<span class="math">\(N\)</span>, ואז לחשב את ממוצע ההטלות. אנחנו עושים את הניסוי הזה בנפרד עבור ערכי <span class="math">\(N\)</span> מ-<span class="math">\(10\)</span> עד <span class="math">\(10^{6}\)</span>, ולכל אחד מהם אנחנו מדפיסים את הממוצע. כשאני עשיתי את זה, קיבלתי: </p>
<div class="highlight"><pre><span></span><code><span class="mi">10</span><span class="p">:</span> <span class="mf">3.1</span>
<span class="mi">100</span><span class="p">:</span> <span class="mf">3.56</span>
<span class="mi">1000</span><span class="p">:</span> <span class="mf">3.372</span>
<span class="mi">10000</span><span class="p">:</span> <span class="mf">3.4951</span>
<span class="mi">100000</span><span class="p">:</span> <span class="mf">3.50129</span>
<span class="mi">1000000</span><span class="p">:</span> <span class="mf">3.499845</span> 
</code></pre></div>

<p>די בבירור, הממוצע מתקרב אל <span class="math">\(3.5\)</span>. זו אבחנה אמפירית; אבל אם אנחנו רוצים לדעת לאן הממוצע ישאף אנחנו <strong>לא צריכים</strong> לבצע ניסויים בפועל - אנחנו יכולים פשוט <strong>לחשב</strong> את זה, ואיכשהו מובטח שהמציאות תסתדר בהתאם לחישוב שלנו. הערך שאנחנו מחשבים הוא - הפתעה הפתעה - התוחלת של המשתנה המקרי שמתאר את הניסוי.</p>
<p>במקרה שלנו, <span class="math">\(X\)</span> מקבל כל ערך בין 1 ל-6 בהסתברות <span class="math">\(\frac{1}{6}\)</span> לכל איבר, אז על פי הגדרה <span class="math">\(E\left[X\right]=\frac{1+2+3+4+5+6}{6}=\frac{21}{6}=3.5\)</span>. אין פה הפתעה, אבל מה שכן יש פה הוא חישוב פשוט למדי, שלא מצריך הטלת מיליון קוביות. והיופי פה הוא שזה עובד <strong>תמיד</strong>, לכל משתנה מקרי <span class="math">\(X\)</span> שאנחנו משכפלים <span class="math">\(n\)</span> עותקים זהים שלו שהם בלתי תלויים אחד בשני ומסתכלים על הממוצע של כולם (טוב, <a href="https://gadial.net/2008/07/25/envelope_paradox/">כמעט תמיד</a>).</p>
<p>לתופעה הזו קוראים <strong>חוק המספרים הגדולים</strong>. יש כמה ניסוחים למשפט שאומר "מה הולך פה" והניסוח הנפוץ הוא בדרך כלל של מה שנקרא <strong>החוק החזק של המספרים הגדולים</strong> שלא אוכיח כרגע אבל כן אסביר מה הוא אומר. אנחנו מתחילים עם סדרה אינסופית של משתנים מקריים <span class="math">\(X_{1},X_{2},X_{3},\ldots\)</span> שכולם מתפלגים באותו האופן (כלומר, בעלי אותה פונקצית צפיפות הסתברות) והם בלתי תלויים, במובן הבא: <span class="math">\(X,Y\)</span> הם בלתי תלויים אם לכל שתי קבוצות אפשריות של תוצאות <span class="math">\(A,B\)</span> מתקיים <span class="math">\(P\left(X\in A\wedge Y\in B\right)=P\left(X\in A\right)P\left(Y\in B\right)\)</span>; ראינו את זה טיפה בפוסט הקודם ולא אתעכב על זה הפעם.</p>
<p>עכשיו, מכיוון שכל המשתנים מתפלגים אותו דבר, יש להם אותה תוחלת. באופן כללי תוחלת עשויה להיות אינסופית או לא מוגדרת, אבל נניח שכאן היא מספר סופי, <span class="math">\(E\left[X_{i}\right]=\mu\)</span>. במקרה הזה אנחנו יכולים להסתכל על המשתנה המקרי <span class="math">\(\overline{X}_{n}=\frac{X_{1}+X_{2}\ldots+X_{n}}{n}\)</span>, שהוא המשתנה שמתאר את הממוצע (הרגיל, הלא משוקלל) של <span class="math">\(n\)</span> התוצאות הראשונות בסדרת המשתנים המקריים. החוק החזק של המספרים הגדולים אומר שמתקיים <span class="math">\(\overline{X}_{n}\to\mu\)</span> בהסתברות 1. בואו נבין טיפה יותר מה זה אומר בעצם.</p>
<p>ה-<span class="math">\(\overline{X}_{n}\)</span>-ים שהגדרנו מתארים <strong>סדרה</strong> של משתנים שמתארת את הממוצעים ההולכים-ומשתפרים: <span class="math">\(\overline{X}_{1},\overline{X}_{2},\overline{X}_{3},\ldots\)</span>. אם אנחנו דוגמים איבר <span class="math">\(a\)</span> כלשהו במרחב המדגם שלנו, הוא ייתן למשתנים <span class="math">\(X_{1},X_{2},\dots\)</span> ערכים קונקרטיים שהם מספרים ממשיים: <span class="math">\(X_{1}\left(a\right),X_{2}\left(a\right),\ldots\)</span>, ולכן גם בסדרת הממוצעים נקבל ערכים קונקרטיים <span class="math">\(\overline{X}_{1}\left(a\right),\overline{X}_{2}\left(a\right),\ldots\)</span>; כלומר, בהינתן שאנחנו מסתכלים על איבר קונקרטי של מרחב המדגם, קיבלנו סדרה "רגילה" של מספרים ממשיים. על סדרה "רגילה" של ממשיים אפשר לשאול האם היא מקיימת את התכונה <span class="math">\(\lim_{n\to\infty}\overline{X}_{n}\left(a\right)=\mu\)</span>, כלומר האם לכל <span class="math">\(\varepsilon&gt;0\)</span> קיים <span class="math">\(N\)</span> כך שלכל <span class="math">\(n&gt;N\)</span> מתקיים <span class="math">\(\left|\overline{X}_{n}\left(a\right)-\mu\right|&lt;\varepsilon\)</span>. לכל <span class="math">\(a\)</span> במרחב המדגם שלנו התשובה לשאלה הזו היא או "כן" או "לא". החוק החזק של המספרים הגדולים אומר שקבוצת כל ה-<span class="math">\(a\)</span>-ים שעבורה התשובה היא "לא" היא ממידה אפס (זה לא אומר שזה לא יכול לקרות; זה אומר שזה זניח).</p>
<p>אני לא אוכיח כאן את חוק המספרים הגדולים; אני בעיקר משתמש בו כדי לתת מוטיבציה לסיבה שבגללה מושג התוחלת קופץ לנו לפרצוף ודורש שנגדיר אותו. יש עוד שימושים דומים לתוחלת, כשהפשוט שבהם הוא כנראה <strong>אי שוויון מרקוב</strong>: אם <span class="math">\(X\)</span> הוא משתנה מקרי שמקבל רק ערכים אי-שליליים ו-<span class="math">\(a&gt;0\)</span> כלשהו, אז</p>
<p><span class="math">\(P\left(X\ge a\right)\le\frac{\text{E}\left[X\right]}{a}\)</span></p>
<p>בניסוח שקול שקצת יותר מאפשר להבין מה הקטע אפשר להכניס את התוחלת לתוך אי השוויון:</p>
<p><span class="math">\(P\left(X\ge a\cdot\text{E}\left[X\right]\right)\le\frac{1}{a}\)</span></p>
<p>כלומר, זה נותן הערכה גסה לשאלה מה הסיכוי ש-<span class="math">\(X\)</span> יהיה גדול מפי 2 התוחלת שלו (פחות מחצי), מפי 3 התוחלת שלו (פחות משליש) וכו'. זו באמת הערכה גסה, אבל לעתים קרובות היא מספיקה כדי להוכיח את מה שרוצים לעשות באותו רגע ולכן, בנוסף לפשטות שלו, אי השוויון הזה הוא כלי שימושי מאוד.</p>
<p>ההוכחה של אי שוויון מרקוב מצחיקה בכמה שהיא פשוטה. אני מגדיר משתנה מקרי <span class="math">\(I\)</span> שהוא מה שנקרא <strong>אינדיקטור</strong>: משתנה מקרי שמקבל או 0 או 1, ולכן בעצם נותן "אינדיקציה" לכך שאירוע מסוים קרה או לא. במקרה שלנו:</p>
<p><span class="math">\(I=\begin{cases} 1 &amp; X\ge a\\ 0 &amp; X&lt;a \end{cases}\)</span></p>
<p>התוחלת של אינדיקטור, ממש על פי ההגדרה, היא ההסתברות שהאירוע שנותן 1 קרה, כלומר <span class="math">\(E\left[I\right]=P\left(X\ge a\right)\)</span>. עכשיו, שימו לב ש-<span class="math">\(I\le\frac{X}{a}\)</span> ותאמינו לי שאפשר פשוט לקחת תוחלת לשני האגפים ולהוציא את הקבוע החוצה, ונקבל</p>
<p><span class="math">\(P\left(X\ge a\right)=E\left[I\right]=\frac{E\left[X\right]}{a}\)</span></p>
<p>אתם כמובן לא אמורים להאמין לי שפשוט אפשר לעשות את זה, אבל אני לא אוכיח את זה הפעם. את הקטע עם "להוציא את הקבוע החוצה" אפשר להכליל - התוחלת מקיימת תכונה מועילה מאוד שנקראת <strong>לינאריות</strong>: אם <span class="math">\(X,Y\)</span> הם משתנים מקריים כלשהם (שיכולים להיות גם תלויים) ו-<span class="math">\(\alpha,\beta\)</span> הם קבועים מספריים, אז <span class="math">\(\text{E}\left[\alpha X+\beta Y\right]=\alpha\text{E}\left[X\right]+\beta\text{E}\left[Y\right]\)</span> (ואפשר להכליל את זה לסכום סופי כלשהו של משתנים מקריים).</p>
<p>הראיתי את המושג האבסטרקטי של תוחלת ואיזו דוגמא מסכנה עם הטלת קוביה, אבל זה לא מספיק, בואו נראה דוגמא קונקרטית - מה שנקרא <strong>התפלגות בינומית</strong>. בהתפלגות בינומית יש לנו ניסוי בסיסי שחוזר על עצמו עם הסתברות <span class="math">\(p\)</span> להצליח ו-<span class="math">\(q=1-p\)</span> להיכשל בכל פעם; כבר ראינו שאם חוזרים על הניסוי <span class="math">\(n\)</span> פעמים אז ההסתברות לקבל בדיוק <span class="math">\(k\)</span> הצלחות היא <span class="math">\(p\left(X=k\right)={n \choose k}p^{k}q^{n-k}\)</span>. לכן, התוחלת של המשתנה המקרי הזה היא</p>
<p><span class="math">\(\text{E}\left[X\right]=\sum_{k=0}^{n}k{n \choose k}p^{k}q^{n-k}\)</span></p>
<p>יש טריק מתמטי לא מסובך שמאפשר לחשב את הסכום הזה, אבל למרבה השמחה הוא עובד באופן קצת יותר כללי ויאפשר לחשב לא רק את <span class="math">\(\text{E}\left[X\right]\)</span> אלא גם את <span class="math">\(\text{E}\left[X^{t}\right]\)</span> לכל חזקה טבעית <span class="math">\(t\)</span>, וזה הולך לעזור לנו בקרוב (הערכים <span class="math">\(\text{E}\left[X^{t}\right]\)</span> נקראים <strong>המומנטים</strong> של המשתנה המקרי <span class="math">\(X\)</span> ויש להם חשיבות באופן כללי). הסכום במקרה הכללי יותר הזה הוא</p>
<p><span class="math">\(\text{E}\left[X^{t}\right]=\sum_{k=0}^{n}k^{t}{n \choose k}p^{k}q^{n-k}\)</span></p>
<p>כי הדרך היחידה שבה הערך <span class="math">\(k^{t}\)</span> יכול להתקבל על ידי המשתנה <span class="math">\(X^{t}\)</span> היא אם הערך <span class="math">\(k\)</span> יתקבל על ידי המשתנה <span class="math">\(X\)</span>, כלומר <strong>ההסתברות</strong> היא אותו דבר וכל מה שהשתנה הוא שמופיע בתחילת הסכום <span class="math">\(k^{t}\)</span> במקום סתם <span class="math">\(k\)</span>.</p>
<p>עכשיו, קודם כל שימו לב שאם <span class="math">\(k=0\)</span> האיבר שמתאים למקרה הזה הוא פשוט אפס, כלומר אפשר לכתוב</p>
<p><span class="math">\(\text{E}\left[X^{t}\right]=\sum_{k=1}^{n}k^{t}{n \choose k}p^{k}q^{n-k}\)</span></p>
<p>הטריק הוא עכשיו לפתוח את ההגדרה של <span class="math">\({n \choose k}=\frac{n!}{k!\left(n-k\right)!}\)</span> ולקבל</p>
<p><span class="math">\(k{n \choose k}=k\cdot\frac{n!}{k!\left(n-k\right)!}=n\frac{k}{k!}\cdot\frac{\left(n-1\right)!}{\left(n-k\right)!}=n\frac{\left(n-1\right)!}{\left(k-1\right)!\left(n-k\right)!}=n{n-1 \choose k-1}\)</span></p>
<p>נציב את זה אצלנו ועל הדרך נוציא <span class="math">\(p\)</span> אחד החוצה כדי להקטין את החזקה של <span class="math">\(p\)</span> באחד:</p>
<p><span class="math">\(\sum_{k=1}^{n}k^{t}{n \choose k}p^{k}q^{n-k}=np\sum_{k=1}^{n}k^{t-1}{n-1 \choose k-1}p^{k-1}q^{n-k}\)</span></p>
<p>עכשיו נבצע החלפת משתנה <span class="math">\(i=k-1\)</span>. כלומר כש-<span class="math">\(k=1\)</span> אז <span class="math">\(i=0\)</span> ואילו כש-<span class="math">\(k=n\)</span> אז <span class="math">\(i=n-1\)</span>, ולכן</p>
<p><span class="math">\(np\sum_{k=1}^{n}k^{t-1}{n-1 \choose k-1}p^{k-1}q^{n-k}=np\sum_{i=0}^{n-1}\left(i+1\right)^{t-1}{n-1 \choose i}p^{i}q^{n-i-1}\)</span></p>
<p>עכשיו בואו נסתכל על הביטוי שקיבלנו בסכום:</p>
<p><span class="math">\(\sum_{i=0}^{n-1}\left(i+1\right)^{t-1}{n-1 \choose i}p^{i}q^{n-i-1}\)</span></p>
<p>חוץ מה-<span class="math">\(\left(i+1\right)^{t-1}\)</span> בהתחלה זה נראה בדיוק כמו סכום על ההסתברויות של משתנה בינומי אחר שאסמן <span class="math">\(Y\)</span>, עם <span class="math">\(n-1\)</span> נסיונות ואותן הסתברויות הצלחה וכישלון <span class="math">\(p,q\)</span>. ה-<span class="math">\(\left(i+1\right)^{t-1}\)</span> בתחילת הסכום נראה קצת מוזר אבל אם חושבים על זה, רואים שהסכום פשוט שווה אל <span class="math">\(\text{E}\left[\left(Y+1\right)^{t-1}\right]\)</span>. כלומר קיבלנו</p>
<p><span class="math">\(\text{E}\left[X^{t}\right]=np\text{E}\left[\left(Y+1\right)^{t-1}\right]\)</span></p>
<p>במקרה הבסיס שבו <span class="math">\(t=1\)</span> אז <span class="math">\(\left(Y+1\right)^{t-1}\)</span> הוא פשוט הקבוע 1 ולכן התוחלת שלו היא 1 ולכן קיבלנו</p>
<p><span class="math">\(\text{E}\left[X\right]=np\)</span></p>
<p>אבל עם הנוסחה נוכל לחשב בקלות גם את <span class="math">\(\text{E}\left[X^{2}\right]\)</span>, למשל. כי:</p>
<p><span class="math">\(\text{E}\left[X^{2}\right]=np\text{E}\left[Y+1\right]=np\left(\text{E}\left[Y\right]+1\right)=np\left(\left(n-1\right)p+1\right)\)</span></p>
<p>כאן למשל השתמשתי בלינאריות של התוחלת. אבל למה בעצם <span class="math">\(\text{E}\left[X^{2}\right]\)</span> מעניין אותי? או, אנחנו בדיוק מגיעים אל זה.</p>
<h2>שונות וסטיית תקן</h2>

<p>ראינו זה עתה שאם <span class="math">\(X\)</span> הוא משתנה בינומי שמתאר ספירה של הצלחות בניסוי שחוזר על עצמו <span class="math">\(n\)</span> פעמים עם הסתברות הצלחה <span class="math">\(p\)</span> בכל פעם, אז התוחלת שלו היא <span class="math">\(np\)</span>. ראינו גם שהתוחלת של התוצאה בהטלת קובייה הוגנת היא <span class="math">\(3.5\)</span>. אז אם נבחר את הפרמטרים של <span class="math">\(X\)</span> להיות <span class="math">\(n=7,p=\frac{1}{2}\)</span> נקבל שהתוחלת של <span class="math">\(X\)</span> היא גם <span class="math">\(3.5\)</span>: התוחלת של "הטל מטבע הוגן 7 פעמים וספור כמה פעמים התקבל עץ" היא אותה תוחלת כמו של "הטל קוביה הוגנת ובדוק מה יצא". אבל אלו בבירור ניסויים שונים למדי זה מזה - בהטלת קוביה לכל תוצאה יש אותו סיכוי, ובניסוי הבינומי אין סימטריה כזו ויש גם ערכים כמו 0 ו-7 שלא יכולים לצוץ בהטלת קוביה. עוד משחק שאני יכול לעשות הוא לשחק "הטלת מטבע" עם מטבע שהערך של צד אחד שלו הוא 0 והערך של הצד השני שלו הוא 7: גם במקרה הזה נקבל תוחלת <span class="math">\(3.5\)</span>.</p>
<p>אם אני לוקח את שלושת הניסויים השונים הללו ועושה עליהם את מה שעשיתי בפוסט הראשון בסדרה, כלומר מצייר את העקומה של ההתפלגות הנורמלית שמקרבת את מה שמקבלים אם חוזרים על הניסוי 100 פעמים וסוכמים את התוצאות - זה מה שאני אקבל:</p>
<p><img src="/new_site/img/2025/histogram_12.png" alt=""/></p>
<p>יש כאן שלוש עקומות נורמליות, שניסיתי לצבוע בצבעים שונים כדי להבדיל ביניהן (אבל אני לא בטוח אם הצבעים ייראו שונים לכל הקוראים). העקומה שמגיעה הכי גבוהה, שצבועה באדום, מתאימה למשתנה הבינומי; האמצעית, שצבועה בכחול, מתאימה למשתנה של הקוביה; והתחתונה, הירוקה, מתאימה למשתנה של המטבע עם הערכים 0 ו-7.</p>
<p>מה הסיבה להבדל? ככל שעקומה נורמלית היא יותר גבוהה, כך זה אומר שרוב ההסתברות שהיא מייצגת מתארת טווח קטן יחסית. המקרה הקיצוני ביותר הוא המשתנה המקרי שפשוט מחזיר <span class="math">\(3.5\)</span> תמיד, שיתואר על ידי קו אנכי בודד בדיוק ב-<span class="math">\(350\)</span>. הסיבה שהעקומה הירוקה, הנמוכה, היא כל כך "רחבה" היא שהניסוי שהיא מתארת הוא בעל רק שני ערכים, ששניהם במרחק לא זניח מהתוחלת עצמה ולכן כל אי-אחידות בתוצאות של ההגרלה (נניח, יתרון של 9 הטלות שנתנו 7 במקום 0) "מושך" את הסכום הכולל רחוק מהממוצע הצפוי. זו האינטואיציה, אבל אנחנו רוצים את המתמטיקה הקונקרטית - מדד מספרי שיאפשר לנו לתאר עד כמה סטייה מהתוחלת היא משהו נפוץ או נדיר.</p>
<p>אם <span class="math">\(X\)</span> הוא משתנה מקרי ו-<span class="math">\(\mu=\text{E}\left[X\right]\)</span> היא התוחלת שלו, אפשר להסתכל על משתנה מקרי חדש, <span class="math">\(Y=X-\mu\)</span>. זה המשתנה המקרי המקורי, רק שעכשיו הוא מנורמל כך שהתוחלת שלו תהיה 0. מכיוון שמה שמעניין אותנו הוא גודל ה"סטייה מהממוצע", והגודל הזה הוא פשוט <span class="math">\(\left|Y\right|\)</span>, אפשר לשאול את עצמנו <strong>מה הסטייה הממוצע מהממוצע</strong>, כלומר <span class="math">\(\text{E}\left[\left|Y\right|\right]\)</span>. זה אמנם מדד מתבקש למדי, אבל הוא פחות מועיל מאשר אפשר היה לקוות. פונקציית הערך המוחלט היא לא "נחמדה" מבחינה מתמטית - בפרט, היא לא גזירה ב-0. עניין אחר הוא שהיא מאותו סדר גודל כמו <span class="math">\(Y\)</span> ודווקא יש עניין בכך שנעבור אל "סדר הגודל הבא". זה מוביל אותנו להגדרה שאולי נראית פחות טבעית ממבט ראשון אבל היא שימושית בצורה בלתי נתפסת: <span class="math">\(\text{Var}\left(X\right)=\text{E}\left[Y^{2}\right]=\text{E}\left[\left(X-\mu\right)^{2}\right]\)</span> כאשר Var כאן הוא קיצור של Variance, <strong>שונות</strong>. העלאה בריבוע נותנת לנו את היתרונות שרצינו: היא משמשת בתור סוג של ערך מוחלט כי <span class="math">\(a^{2}\)</span> זה אותו דבר כמו <span class="math">\(\left|a\right|^{2}\)</span>; זה עובר לסדר הגודל השני (חשבו על העלאה בריבוע בתור העלאה של סדר הגודל; איך זה באמת חשוב, נראה אחר כך). וזו פונקציה נחמדה לעבודה מבחינה מתמטית.</p>
<p>דבר אחד שקל לראות הוא נוסחה אלטרנטיבית לתוחלת שמתקבלת מכך שפותחים במפורש את הביטוי, מה שאני אדגים במקרה של הסתברות בדידה:</p>
<p><span class="math">\(\text{Var}\left(X\right)=\text{E}\left[\left(X-\mu\right)^{2}\right]=\sum_{a}p\left(X=a\right)\left(a-\mu\right)^{2}=\)</span></p>
<p><span class="math">\(=\sum_{a}P\left(X=a\right)a^{2}-2\mu\sum_{a}P\left(X=a\right)a+\mu^{2}\sum_{a}P\left(X=a\right)=\)</span></p>
<p><span class="math">\(=\text{E}\left[X^{2}\right]-2\mu\text{E}\left[X\right]+\mu^{2}=\text{E}\left[X^{2}\right]-2\mu^{2}+\mu^{2}=\text{E}\left[X^{2}\right]-\mu^{2}=\text{E}\left[X^{2}\right]-\text{E}\left[X\right]^{2}\)</span></p>
<p>בואו נוודא שהבנו מה הולך פה. את הביטוי המקורי פיצלתי לשלושה סכומים על פי הפתיחה של הסוגריים <span class="math">\(\left(a-\mu\right)^{2}=a^{2}-2a\mu+\mu^{2}\)</span>. הסכום הראשון היה <span class="math">\(\sum_{a}P\left(X=a\right)a^{2}\)</span> שזו ממש ההגדרה הפורמלית של <span class="math">\(\text{E}\left[X^{2}\right]\)</span>. הסכום השני היה קבוע כפול <span class="math">\(\sum_{a}P\left(X=a\right)a\)</span> שזו ההגדרה הפורמלית של <span class="math">\(\text{E}\left[X\right]\)</span>, ואחרי שהשתמשתי בכך ש-<span class="math">\(\text{E}\left[X\right]=\mu\)</span> קיבלתי את הערך שרציתי. </p>
<p>הנוסחה <span class="math">\(\text{Var}\left(X\right)=\text{E}\left[X^{2}\right]-\text{E}\left[X\right]^{2}\)</span> בהחלט נראית לי יפה ושימושית יותר מאשר <span class="math">\(\text{E}\left[\left(X-\mu\right)^{2}\right]\)</span>; היא מציגה את השונות בעזרת שני המומנטים הראשונים של <span class="math">\(X\)</span>, שיותר קל לחשב ישירות. עשיתי את זה קודם, בדוגמה למעלה, עבור התפלגות בינומית: קיבלנו ש-<span class="math">\(\text{E}\left[X\right]=np\)</span> ואילו <span class="math">\(\text{E}\left[X^{2}\right]=np\left(\left(n-1\right)p+1\right)\)</span>, ולכן עכשיו אני אוכל לחשב בקלות את השונות של התפלגות בינומית:</p>
<p><span class="math">\(\text{Var}\left(X\right)=np\left(\left(n-1\right)p+1\right)-\left(np\right)^{2}=np\left[\left(n-1\right)p+1-np\right]=\)</span></p>
<p><span class="math">\(=np\left(np-p+1-np\right)=np\left(1-p\right)=npq\)</span></p>
<p>ובסופו של דבר קיבלנו ביטוי פשוט ואלגנטי מאוד עבור השונות. עבור המשתנה הבינומי הקונקרטי שתיארתי למעלה, של ספירת העצים ב-7 הטלות של מטבע הוגן, הפרמטרים היו <span class="math">\(n=7,p=q=0.5\)</span> ולכן התוחלת היא <span class="math">\(\frac{7}{2}=3.5\)</span> כפי שכבר ראינו, והשונות היא <span class="math">\(\frac{7}{4}=1.75\)</span>.</p>
<p>עבור תוחלת ראיתי שמתקיימת תכונת <strong>לינאריות</strong> מועילה מאוד: <span class="math">\(\text{E}\left[\alpha X+\beta Y\right]=\alpha\text{E}\left[X\right]+\beta\text{E}\left[Y\right]\)</span>. עבור שונות אין לנו משהו עד כדי כך נוח, אבל עדיין יש לנו נוסחה מועילה: <span class="math">\(\text{Var}\left(\alpha X+\beta\right)=\alpha^{2}\text{Var}\left(X\right)\)</span>. כלומר - חיבור של <strong>קבוע</strong> לא משפיע (חיבור של משתנה מקרי אחר מסבך את הכל), והכפלה בקבוע מכילה את השונות באותו קבוע בריבוע.</p>
<p>עכשיו, העובדה שהשונות כוללת העלאה בריבוע היא מצד אחד חיובית מהסיבות שציינתי, ומצד שני אנחנו עדיין רוצים לקבל מדד דומה שהוא "מסדר גודל אחד פחות" ובכל זאת מתנהג נחמד. דרך פשוטה לקבל את זה היא להגדיר <span class="math">\(\sigma=\sqrt{\text{Var}\left(X\right)}\)</span> - גם פונקציית השורש היא נחמדה יחסית, ומורידה את סדר הגודל. ה-<span class="math">\(\sigma\)</span> הזה חשוב מספיק כדי שיקבל שם בפני עצמו: <strong>סטיית התקן</strong> של <span class="math">\(X\)</span>, ולעתים קרובות יותר נוח פשוט לכתוב <span class="math">\(\sigma^{2}\)</span> בתור השונות של <span class="math">\(X\)</span> במקום <span class="math">\(\text{Var}\left(X\right)\)</span>. את התוצאה שראינו לפני רגע אפשר לתאר גם בתור "אם כופלים את <span class="math">\(X\)</span> ב-<span class="math">\(\alpha\)</span> כופלים את סטיית התקן של <span class="math">\(X\)</span> ב-<span class="math">\(\alpha\)</span>".</p>
<p>בואו נראה עכשיו שימוש בשונות כדי לשפר את היכולת שלנו להבין את <span class="math">\(X\)</span> בעזרת מעין שדרוג של אי-שוויון מרקוב שלוקח גם את השונות בחשבון ונקרא <strong>אי-שוויון צ'בישב</strong>.</p>
<p>הרעיון הוא כזה. כזכור, אי-שוויון מרקוב מדבר על משתנה מקרי <strong>אי-שלילי</strong> <span class="math">\(X\)</span> ועבורו, לכל <span class="math">\(a&gt;0\)</span> , הוא נותן</p>
<p><span class="math">\(P\left(X\ge a\right)=\frac{E\left[X\right]}{a}\)</span></p>
<p>עכשיו, אם <span class="math">\(X\)</span> הוא משתנה מקרי <strong>כלשהו</strong>, לאו דווקא אי שלילי, אז בואו ניזכר ש-<span class="math">\(\text{Var}\left(X\right)\)</span> הוגדרה בתור התוחלת של משתנה מקרי שהוא <strong>כן</strong> אי-שלילי: <span class="math">\(\sigma^{2}=\text{Var}\left(X\right)=\text{E}\left[Z\right]\)</span> כאשר <span class="math">\(Z=\left(X-\mu\right)^{2}\)</span>. על ה-<span class="math">\(Z\)</span> הזה אני כן יכול להשתמש באי שוויון מרקוב; בואו נראה מה קורה אם אנחנו לוקחים <span class="math">\(k&gt;0\)</span> כלשהו ומשתמשים במרקוב עם <span class="math">\(a=k^{2}\)</span>:</p>
<p><span class="math">\(P\left(Z\ge a\right)\le\frac{\text{E}\left[Z\right]}{a}\)</span></p>
<p>כלומר</p>
<p><span class="math">\(P\left(\left(X-\mu\right)^{2}\ge k^{2}\right)\le\frac{\text{E}\left[\left(X-\mu\right)^{2}\right]}{k^{2}}=\frac{\sigma^{2}}{k^{2}}\)</span></p>
<p>עכשיו, בואו נביט על <span class="math">\(\left(X-\mu\right)^{2}\ge k^{2}\)</span>. הדבר הזה קורה אם ורק אם <span class="math">\(\left|X-\mu\right|\ge k\)</span>, ולכן ההסתברות של שני אי השוויונים זהה, כלומר אני יכול לכתוב</p>
<p><span class="math">\(P\left(\left|X-\mu\right|\ge k\right)\le\frac{\sigma^{2}}{k^{2}}\)</span></p>
<p>זה אי-שוויון צ'בישב. כמו עם אי-שוויון מרקוב, לפעמים קל יותר להבין מה הוא אומר אם "מכניסים את סטיית התקן אל אי השוויון", כלומר אם אני מנסח אותו על ידי</p>
<p><span class="math">\(P\left(\left|X-\mu\right|\ge k\sigma\right)\le\frac{1}{k^{2}}\)</span></p>
<p>אם למשל נציב <span class="math">\(k=2\)</span> נקבל את הטענה הבאה: לכל משתנה מקרי <span class="math">\(X\)</span>, הסיכוי שהוא יקבל ערך שהמרחק שלו מהתוחלת של <span class="math">\(X\)</span> הוא יותר משתי סטיות תקן הוא <span class="math">\(\frac{1}{4}=25\%\)</span>. לי זה מרגיש כמו טענה כללית להפתיע, כי לא משנה בכלל כמה <span class="math">\(X\)</span> מורכב ומתוחכם - מרגע שחישבנו את התוחלת ואת סטיית התקן שלו, אנחנו יודעים בדיוק מה האיזור שבו 75\% מהערכים ש-<span class="math">\(X\)</span> נותן הולכים ליפול. יותר מכך - בגלל שבמכנה יש <span class="math">\(k^{2}\)</span> ולא <span class="math">\(k\)</span>, זה אומר לנו שהסיכוי של מישהו להיות במרחק של הרבה סטיות תקן צונח מהר - צונח באופן <strong>ריבוע</strong>. אם הסיכוי להיות במרחק 2 סטיות תקן היה 25\%, עבור 3 סטיות תקן הוא נופל אל <span class="math">\(\frac{1}{9}=11.111\ldots\%\)</span>, ועבור 4 סטיות תקן הוא כבר <span class="math">\(\frac{1}{16}=6.25\%\)</span> וכשנרצה להיות במרחק 10 סטיות תקן הסיכוי כבר יהיה <span class="math">\(\frac{1}{100}=1\%\)</span>, וכן הלאה. ככל שסטיית התקן של <span class="math">\(X\)</span> גדולה יותר כך האיזור שבו רוב הערכים שלו יכולים ליפול הוא רחב יותר, אבל אם נחלק את המרחב לאיזורים שהרוחב של כל אחד מהם הוא <span class="math">\(\sigma\)</span>, נדע בדיוק מה ההסתברות של איבר ליפול בכל אחד מהאיזורים הללו. זו המחשה חזקה מאוד לכוח שיש לשני מספרים בודדים <span class="math">\(\mu,\sigma\)</span> לתאר את <span class="math">\(X\)</span>.</p>
<h2>חזרה אל ההתפלגות הנורמלית</h2>

<p>עכשיו כשהכרנו את המושגים של תוחלת ושונות/סטיית תקן אפשר לחזור סוף סוף אל ההתפלגות הנורמלית. הגדרתי אותה בעזרת פונקציית צפיפות ההסתברות <span class="math">\(f\left(x\right)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\left(x-\mu\right)^{2}/2\sigma^{2}}\)</span>, ועכשיו אפשר לחזור למה שאמרתי בתחילת הפוסט אבל כשיש לנו את ההגדרות הרלוונטיות: ה-<span class="math">\(\mu\)</span> וה-<span class="math">\(\sigma\)</span> הללו הם בדיוק התוחלת וסטיית התקן של ההתפלגות הנורמלית ש-<span class="math">\(\mu,\sigma\)</span> הם הפרמטרים שלה. כלומר, אנחנו מגדירים משתנה מקרי <span class="math">\(N\left(\mu,\sigma\right)\)</span> על ידי פונקציית הצפיפות <span class="math">\(f\left(x\right)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\left(x-\mu\right)^{2}/2\sigma^{2}}\)</span>, וכשנחשב את התוחלת וסטיית התקן שלו הנוסחאות יתנו לנו בדיוק <span class="math">\(\mu,\sigma\)</span>, וזה בדיוק מה שאני רוצה להראות עכשיו. דבר אחד שכדאי לשים אליו לב לפני כן הוא שאפשר להגדיר את ההתפלגות הנורמלית גם בלי להגדיר בכלל סטיית תקן: הרי <span class="math">\(\frac{1}{\sqrt{2\pi}\sigma}e^{-\left(x-\mu\right)^{2}/2\sigma^{2}}=\frac{1}{\sqrt{2\pi\sigma^{2}}}e^{-\left(x-\mu\right)^{2}/2\sigma^{2}}\)</span> ולכן אם היינו למשל בוחרים לסמן את השונות שאנחנו רוצים ב-<span class="math">\(\tau\)</span> אז היינו מקבלים שהתפלגות נורמלית עם תוחלת <span class="math">\(\mu\)</span> ושונות <span class="math">\(\tau\)</span> מתוארת על ידי <span class="math">\(f\left(x\right)=\frac{1}{\sqrt{2\pi\tau}}e^{-\left(x-\mu\right)^{2}/2\tau}\)</span>. הבחירה להשתמש ב-<span class="math">\(\sigma\)</span> היא עניין של קונבנציה (ומן העבר השני, אנשים גם כותבים לפעמים <span class="math">\(\frac{1}{\sqrt{2\pi}\sigma}e^{-\left(\frac{x-\mu}{\sigma}\right)^{2}/2}\)</span> כדי שהנוסחה תכלול רק את <span class="math">\(\sigma\)</span> ולא את <span class="math">\(\sigma^{2}\)</span>).</p>
<p>ראשית, הנה טריק מועיל כדי למצוא תוחלת ושונות של דברים. כזכור, ראינו</p>
<p><span class="math">\(\text{E}\left[\alpha X+\beta Y\right]=\alpha\text{E}\left[X\right]+\beta\text{E}\left[Y\right]\)</span></p>
<p><span class="math">\(\text{Var}\left(\alpha X+\beta\right)=\alpha^{2}\text{Var}\left(X\right)\)</span></p>
<p>אז באופן כללי, אם אנחנו רוצים למצוא תוחלת ושונות של משתנה <span class="math">\(X\)</span> אבל זה קשה לנו, אפשר לנסות להגדיר משתנה "מנורמל" <span class="math">\(Z=\frac{X-\mu}{\sigma}\)</span> ולקבל</p>
<p><span class="math">\(\text{E}\left[X\right]=\sigma\text{E}\left[Z\right]+\mu\)</span></p>
<p><span class="math">\(\text{Var}\left(X\right)=\sigma^{2}\text{Var}\left(Z\right)\)</span></p>
<p>אני אשתמש בזה במקרה שלנו. נניח ש-<span class="math">\(X\)</span> הוא משתנה מקרי שמתפלג נורמלית עם פרמטרים <span class="math">\(\mu,\sigma\)</span>, כלומר <span class="math">\(f\left(x\right)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\left(\frac{x-\mu}{\sigma}\right)^{2}/2}\)</span>. אם אני מגדיר את המשתנה החדש <span class="math">\(Z=\frac{X-\mu}{\sigma}\)</span> אני אקבל פונקציית צפיפות הסתברות <span class="math">\(f\left(z\right)=\frac{1}{\sqrt{2\pi}}e^{-z^{2}/2}\)</span>. כדי לראות את זה נשים לב לכך ש:</p>
<p><span class="math">\(P\left(a\le Z\le b\right)=P\left(\sigma a+\mu\le X\le\sigma b+\mu\right)=\int_{\sigma a+\mu}^{\sigma b+\mu}\frac{1}{\sqrt{2\pi}\sigma}e^{-\left(\frac{x-\mu}{\sigma}\right)^{2}/2}dx\)</span></p>
<p>ועל האינטגרל הזה אפשר לבצע את החלפת המשתנים <span class="math">\(z=\frac{x-\mu}{\sigma}\)</span> שנותנת לנו <span class="math">\(dz=\frac{dx}{\sigma}\)</span>, כלומר מקבלים <span class="math">\(P\left(a\le Z\le b\right)=\frac{1}{\sqrt{2\pi}}e^{-z^{2}/2}\)</span>.</p>
<p>המסקנה? אם <span class="math">\(X\)</span> התפלג נורמלית עם פרמטרים <span class="math">\(\mu,\sigma\)</span> אז <span class="math">\(Z=\frac{X-\mu}{\sigma}\)</span> מתפלג נורמלית עם פרמטרים <span class="math">\(0,1\)</span>. עכשיו אני אוכיח ש-<span class="math">\(\text{E}\left[Z\right]=0\)</span> וש-<span class="math">\(\text{Var}\left(Z\right)=1\)</span> ואז אסיק ש-</p>
<p><span class="math">\(\text{E}\left[X\right]=\sigma\text{E}\left[Z\right]+\mu=\mu\)</span></p>
<p><span class="math">\(\text{Var}\left(X\right)=\sigma^{2}\text{Var}\left(Z\right)=\sigma^{2}\)</span></p>
<p>וזו הטענה הכללית שאני רוצה.</p>
<p>עכשיו, <span class="math">\(\text{E}\left[Z\right]\)</span> זה קל - זה יוצא 0 בגלל שפונקציית הצפיפות <span class="math">\(\frac{1}{\sqrt{2\pi}}e^{-z^{2}/2}\)</span> היא סימטרית ביחס ל-0 כי <span class="math">\(z^{2}\)</span> היא פונקציה סימטרית ביחס ל-0. זה הופך את האינטגרל שמחשבים עבור התוחלת לאינטגרל של פונקציה <strong>אנטיסימטרית</strong> בגלל ההכפלה ב-<span class="math">\(z\)</span>. הנה משהו פורמלי קצת יותר:</p>
<p><span class="math">\(\text{E}\left[Z\right]=\int_{-\infty}^{\infty}z\frac{1}{\sqrt{2\pi}}e^{-z^{2}/2}=\lim_{a\to\infty}\int_{-a}^{a}z\frac{1}{\sqrt{2\pi}}e^{-z^{2}/2}=\)</span></p>
<p><span class="math">\(=\frac{1}{\sqrt{2\pi}}\lim_{a\to\infty}\left.e^{-z^{2}/2}\right|_{-a}^{a}=\frac{1}{\sqrt{2\pi}}\lim_{a\to\infty}\left(e^{-a^{2}/2}-e^{-a^{2}/2}\right)=\)</span></p>
<p><span class="math">\(\frac{1}{\sqrt{2\pi}}\lim_{a\to\infty}0=0\)</span></p>
<p>המעבר מאינטגרל אינסופי בשני הכיוונים לגבול כש-<span class="math">\(a\)</span> שואף לאינסוף של אינטגרל מ-<span class="math">\(-a\)</span> אל <span class="math">\(a\)</span> הוא <strong>לא תקין</strong> באופן כללי אבל בפוסט הקודם כבר אמרנו שבמקרה הנוכחי זה תקין (אבל אפשר להסתדר גם אם עושים שני אינטגרלים, זה פשוט יוצא מסורבל יותר). אז זה מסיים עם התוחלת ומראה שכדי לחשב את השונות צריך פשוט לחשב את <span class="math">\(\text{E}\left[Z^{2}\right]\)</span>. זה יהיה טיפה יותר מסובך ויצריך טריק אינטגרציה סטנדרטי: <strong>אינטגרציה בחלקים</strong>. בואו ניזכר את הטריק הזה הולך.</p>
<p>עבור נגזרות, אחת מהנוסחאות החשובות ביותר היא מה יוצאת הגזירה של מכפלה של פונקציות: <span class="math">\(\left(uv\right)^{\prime}=u^{\prime}v+uv^{\prime}\)</span>. זו אחת מהנוסחאות הבודדות שצרובות בי עוד מהתיכון. עכשיו, אם מעבירים אגפים, מקבלים מן הסתם <span class="math">\(u^{\prime}v=\left(uv\right)^{\prime}-uv^{\prime}\)</span> ואם (ועכשיו אני עובר לקצת נפנוף ידיים) לוקחים אינטגרל לשני האגפים מקבלים <span class="math">\(\int u^{\prime}v=uv-\int uv^{\prime}\)</span>. לכן אם קשה לנו לבצע אינטגרציה לפונקציה שלנו, אנחנו בודקים אם יש דרך נוחה לפרק אותה למכפלה של שתי פונקציות שלאחת מהן (שאנחנו קוראים לה <span class="math">\(u^{\prime}\)</span>) כן קל לנו להוציא אינטגרל, את השניה (שאנחנו קוראים לה <span class="math">\(v\)</span>) קל לנו לגזור, ואז אנחנו מקווים שאחרי שנוציא אינטגרל לאחת ונגזרת לשניה, המכפלה של <strong>זה</strong> תהיה משהו שקל לנו למצוא לו אינטגרל. לפעמים זה נכשל קטסטרופלית ולפעמים, כמו עכשיו, זה עובד יופי. הרי </p>
<p><span class="math">\(\text{E}\left[Z^{2}\right]=\int_{-\infty}^{\infty}z^{2}\frac{1}{\sqrt{2\pi}}e^{-z^{2}/2}\)</span></p>
<p>ולפני רגע, בחישוב התוחלת, ראינו שקל לנו להוציא את האינטגרל של <span class="math">\(u^{\prime}=z\frac{1}{\sqrt{2\pi}}e^{-z^{2}/2}\)</span> והאינטגרל יוצא <span class="math">\(u=\frac{1}{\sqrt{2\pi}}e^{-z^{2}/2}\)</span>, וכמובן שאת <span class="math">\(v=z\)</span> קל לגזור ולקבל <span class="math">\(v^{\prime}=1\)</span>. כמו כן <span class="math">\(uv\)</span> הולך לצאת בדיוק הביטוי לתוחלת שראינו לפני רגע שהאינטגרל שלו הוא 0, ו-<span class="math">\(uv^{\prime}\)</span> הולך לצאת בדיוק פונקציית הצפיפות המקורית שראינו כבר (בפוסט הקודם, ולא בקלות) שהאינטגרל שלה יוצא 1. זה מסיים (בנפנוף ידיים) את ההוכחה.</p>
<p>אז אם לסכם עוד סיכום ביניים: אנחנו מנסים להבין למה פונקציית הצפיפות של ההתפלגות הנורמלית נראית ככה: <span class="math">\(f\left(x\right)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\left(x-\mu\right)^{2}/2\sigma^{2}}\)</span>. בינתיים הבנו ש-<span class="math">\(\pi\)</span> שם בתור קבוע נרמול שמגיע בבסיסו מכך ש-<span class="math">\(2\pi\)</span> הוא היקף מעגל היחידה; ואנחנו רואים עכשיו ש-<span class="math">\(\mu,\sigma\)</span> שמופיעים פה מבטיחים שהתוחלת וסטיית התקן של ההתפלגות יהיו <span class="math">\(\mu,\sigma\)</span>; אבל עדיין לא ברור לנו הדבר העיקרי - מה הקטע עם ה-<span class="math">\(e\)</span> הזה? למה הצורה של העקומה היא (עד כדי קבועים) <span class="math">\(e^{-x^{2}}\)</span>? אני לא רואה איך לענות לשאלה הזו בלי להגיע אל היעד שלנו: <strong>משפט הגבול המרכזי</strong>. את זה נעשה בפוסט הבא.</p>
        </article>
        
        <footer>
            <p>© כל הזכויות שמורות לגדי אלכסנדרוביץ'</p>
        </footer>
    </div>

    
    <script>
        function searchPosts() {
            const searchTerm = document.getElementById('searchInput').value;
            if (searchTerm.trim()) {
                window.location.href = '/new_site/post_list.html?search=' + encodeURIComponent(searchTerm);
            }
        }
        
        document.getElementById('searchInput').addEventListener('keypress', function(e) {
            if (e.key === 'Enter') {
                searchPosts();
            }
        });
    </script>
    
    
    <!-- Auto-render KaTeX math -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "\\[", right: "\\]", display: true},
                    {left: "\\(", right: "\\)", display: false}
                ],
                throwOnError: false
            });
        });
    </script>

</body>
</html>
---
title: "על עקומות ואינטגרלים"
layout: post
categories:
  - אנליזה מתמטית
tags:
  - אינטגרל קווי
---


<h2>מבוא</h2>

הפוסט הזה נולד מהרצון שלי לכתוב פוסט על <strong>אינטגרל מרוכב</strong>, שהוא נושא יפהפה ומרתק שנפתח כמובן עם הגדרה. ההגדרה בסיסית ומוכרת ואף אחד בעולם לא חולק עליה ולכן כמובן שהייתי צריך <strong>לשכנע את עצמי</strong> שהיא מוצדקת, מה שהוביל אותי למחילת ארנב: האינטגרל המרוכב הוא סוג של <strong>אינטגרל קווי</strong>, ולכן חזרתי אל הפוסט שלי על <a href="https://gadial.net/2016/04/19/line_integral/">אינטגרל קווי</a> כדי לראות מה כתבתי אז, כשהעמקתי לעובי הקורה. מה חשכו עיני כשגיליתי שבחלקים מסוימים חיפפתי ובחלקים אחרים <strong>פשוט טעיתי</strong>. אז הפוסט הזה כאן כדי לתקן (אל תחפשו לי טעויות בפוסט ההוא; פשוט הורדתי אותן וקישרתי לפוסט הזה במקום זאת למי שרוצים להעמיק).

מה אני הולך לעשות בפוסט הזה? מטרת העל שלי היא להסביר אינטגרלים קוויים (יש שני סוגים, אינטגרל קווי מסוג ראשון ואינטגרל קווי מסוג שני ולמרות שהם דומים עדיין יש מספיק מה לדבר על כל אחד בנפרד). בשביל זה אני ארצה קודם כל להזכיר מה זה בכלל אינטגרל, ולא פחות חשוב - נצטרך להסביר מה זה בכלל "קו" - או ליתר דיוק, מה זו <strong>עקומה</strong>. הלב הטכני של הפוסט הזה מתחבא לדעתי בכלל בנוסחה שנותנת לנו אורך של עקומה באמצעות אינטגרל, כלומר נגיע אל הלב הטכני עוד לפני שנציג אינטגרלים קוויים. בנוסף, אני מתכנן להיות פדנט אפילו יותר מהרגיל - למרות שהבלוג נקרא "לא מדויק" זה מהפוסטים הללו שבהם אני מרגיש שאני פשוט <strong>חייב</strong> להיות מדויק עד הסוף אחרת אני ארגיש שאני בכלל לא מבין את הנושא (על מי אני עובד? אני באמת לא מבין את הנושא כרגע! אני אולי אבין אותו רק אחרי שאסיים לכתוב את הפוסט הזה, ולא משנה כמה ספרים כבר קראתי על הנושא).

אז בואו נתחיל מההתחלה.

<h2>אינטגרל רימן</h2>

ראשית, מה זה בכלל אינטגרל? בהינתן פונקציה {% equation %}f{% endequation %} ותחום מסוים שהיא מוגדרת בו, אפשר לחשוב על אינטגרל בתור סכום משוקלל של הערכים של הפונקציה בכל התחום. אם הפונקציה קבועה, האינטגרל שלה על התחום צריך להיות שווה לאורך/שטח/נפח שלו (<strong>המידה</strong> שלו, אם קופצים למושג מתמטי שלא אשתמש בו כאן). האופן שבו עושים את זה הוא על ידי קירובים שהולכים ומשתפרים, כפי שתמיד עושים באינפי.

האינטגרל הבסיסי ביותר הוא <strong>אינטגרל רימן</strong> ויש שתי שיטות סטנדרטיות להגדיר אותו שנותנות בסופו של דבר את אותו הדבר. שיטה מקובלת אחת משתמשת במשהו שנקרא <strong>סכומי דארבו</strong> וזו דרך די יפה ואלגנטית ואני לא אשתמש בה כאן. השניה, <strong>סכומי רימן</strong>, תתאים הרבה יותר למה שאני רוצה לעשות. באינטגרל רימן יש לנו פונקציה {% equation %}f:\left[a,b\right]\to\mathbb{R}{% endequation %} ואנחנו רוצים להגדיר את הביטוי {% equation %}\int_{a}^{b}f\left(t\right)dt{% endequation %}. הרעיון הוא להגדיר אותו בעזרת <strong>קירובים</strong>: במקום לחשב סכום אינסופי, אנחנו מחלקים את הקטע {% equation %}\left[a,b\right]{% endequation %} למספר סופי של קטעים, בוחרים נקודה שרירותית בכל אחד מהקטעים, וסוכמים את הערך של {% equation %}f{% endequation %} על נקודה כזו באורך הקטע שבו הנקודה נמצאת. התקווה היא שככל שהקטעים הופכים לקטנים יותר ויותר, כך הסכום שנקבל יתקרב יותר ויותר אל משהו ספציפי; {% equation %}\int_{a}^{b}f\left(t\right)dt{% endequation %} יוגדר להיות המשהו הספציפי הזה.

אם כן, בואו נגדיר <strong>חלוקה</strong> של {% equation %}\left[a,b\right]{% endequation %}. נסמן חלוקה כזו באות {% equation %}P{% endequation %} (מלשון Partition) והיא כוללת סדרה של נקודות {% equation %}a=t_{0}<t_{1}<t_{2}<\ldots<t_{n}=b{% endequation %} שאנחנו חושבים עליהן כמגדירות סדרה של {% equation %}n{% endequation %} קטעים: {% equation %}\left[t_{0},t_{1}\right],\left[t_{1},t_{2}\right],\ldots,\left[t_{n-1},t_{n}\right]{% endequation %}. האיחוד של כל הקטעים הללו נותן את הקטע {% equation %}\left[a,b\right]{% endequation %} המקורי. בשביל לפרמל את "הקטעים הופכים לקטנים יותר ויותר" אני מסמן {% equation %}\Delta t_{i}=t_{i}-t_{i-1}{% endequation %} ומגדיר לכל חלוקה {% equation %}P{% endequation %} את <strong>פרמטר החלוקה</strong> {% equation %}\lambda\left(P\right)=\max\left\{ \Delta t_{i}\right\} _{i=1}^{n}{% endequation %}, האורך של הקטע הארוך ביותר בחלוקה.

עכשיו כשיש לנו חלוקה אפשר לבחור באופן שרירותי נקודות מכל קטע שלה, וליצור את מה שקראתי לו סכום רימן: אז בוחרים סדרה {% equation %}t_{1}^{*},\ldots,t_{n}^{*}{% endequation %} של נקודות כך ש-{% equation %}t_{i}^{*}\in\left[t_{i-1},t_{i}\right]{% endequation %} ואז בונים את הסכום {% equation %}S_{P}=\sum_{i=1}^{n}f\left(t_{i}^{*}\right)\Delta t_{i}{% endequation %} של הערכים של {% equation %}f{% endequation %} בנקודות שבחרתי בתוך כל קטע, כפול אורך הקטע הזה. שימו לב שב-{% equation %}S_{P}{% endequation %} מופיעה החלוקה {% equation %}P{% endequation %} אבל לא טרחתי לציין במפורש את סדרת ה-{% equation %}t_{i}^{*}{% endequation %}-ים שבחרתי; אינטואיטיבית זה בגלל שעבור <strong>כל</strong> בחירות נקודות בתוך החלוקה {% equation %}P{% endequation %} אמור להתקיים אותו דבר נחמד.

מה הדבר הנחמד? הנה ההגדרה הפורמלית לאינטגרל שנעזרת בסכומי רימן: אם קיים מספר ממשי {% equation %}I\in\mathbb{R}{% endequation %} כך שלכל {% equation %}\varepsilon>0{% endequation %} קיים {% equation %}\delta>0{% endequation %} כך שעבור כל חלוקה {% equation %}P{% endequation %} שמקיימת {% equation %}\lambda\left(P\right)<\delta{% endequation %} וכל סכום רימן {% equation %}S_{P}{% endequation %} שמתאים לחלוקה הזו, מתקיים {% equation %}\left|S_{P}-I\right|<\varepsilon{% endequation %}, אז אומרים ש-{% equation %}\int_{a}^{b}f\left(t\right)dt{% endequation %} מוגדר ו-{% equation %}\int_{a}^{b}f\left(t\right)dt=I{% endequation %}.

זו הייתה הגדרה טיפה ארוכה ומפותלת, אבל אין כאן שום דבר מורכב במיוחד למי שכבר התרגלו להגדרות {% equation %}\varepsilon-\delta{% endequation %} בחדו"א, אז אני לא אתעכב עליה יותר מזה.

<h2>עקומות והאורך שלהן</h2>

הרעיון באינטגרלים קוויים הוא לבצע אינטגרציה שבה התחום הוא <strong>עקומה</strong> שחיה ב-{% equation %}\mathbb{R}^{n}{% endequation %}. מה זו עקומה? אינטואיטיבית זו קבוצת נקודות ב-{% equation %}\mathbb{R}^{n}{% endequation %} שנראית כמו קו חד ממדי, אבל כזה שיכול להסתובב ולהתפתל - תחשבו על חוט מתוח שאנחנו נותנים לחתול להתפרע איתו. כמובן, אנחנו לא רוצים להתפרע <strong>יותר מדי</strong> - אסור לקו הזה להיקרע, או לבצע סיבובים חדים מדי; הדרך שלנו לפרמל את זה היא להגדיר עקומה בתור פונקציה {% equation %}\gamma:\left[a,b\right]\to\mathbb{R}^{n}{% endequation %} שהיא גזירה ברציפות, מה שנקרא <strong>חלקה</strong>. אפשר לדמיין את מה ש-{% equation %}\gamma{% endequation %} עושה בתור לקחת את הקטע {% equation %}\left[a,b\right]{% endequation %}, לשתול אותו במרחב {% equation %}\mathbb{R}^{n}{% endequation %} ולעקם ולפתל אותו כמו חתול, בלי לקרוע - האובייקט שמתקבל הוא עדיין חד ממדי. אפשר לדמיין את מה ש-{% equation %}\gamma{% endequation %} עושה גם בתור "טיול על העקומה": יש לנו משתנה {% equation %}a\le t\le b{% endequation %} שמתאר את הזמן הנוכחי של הטיול, שמתחיל בזמן {% equation %}a{% endequation %} ומסתיים בזמן {% equation %}b{% endequation %}, ו-{% equation %}\gamma\left(t\right){% endequation %} אומר איפה בתוך {% equation %}\mathbb{R}^{n}{% endequation %} אנחנו נמצאים בדיוק בזמן {% equation %}t{% endequation %} של הטיול. 

שימו לב ש-{% equation %}\gamma{% endequation %} לא בדיוק מתארת קבוצת נקודות במישור - היא מתארת <strong>טיול</strong> על הקבוצה הזו. אפשר לסמן את הקבוצה הזו בסימון קונקרטי - {% equation %}C=\gamma\left(\left[a,b\right]\right){% endequation %} ואני לרוב קורא ל-{% equation %}C{% endequation %} <strong>עקום</strong>. כלומר - העקום הוא אוסף הנקודות עצמו, העקומה היא דרך אפשרית אחת לתאר אותו (לפעמים גם משתמשים בביטוי <strong>פרמטריזציה של העקום</strong> כדי לתאר את {% equation %}\gamma{% endequation %}). אפשר להוכיח שלא משנה איזו {% equation %}\gamma{% endequation %} נבחר עבור {% equation %}C{% endequation %} - כל עוד בחרנו {% equation %}\gamma{% endequation %} "נחמדה מספיק", תמיד נקבל את אותו ערך של אינטגרל - אבל אני לא אכנס לזה כאן. 

לפני שאני מתחיל להשתמש בעקומות כדי לתאר אינטגרלים, יש שתי שאלות שאני רוצה לענות עליהן:

<ol> <li>איך <strong>מגדירים</strong> את האורך של עקומה?</li>


<li>איך <strong>מחשבים</strong> את האורך של עקומה?</li>

</ol>

עבור 1 אפשר לתת הגדרה פשוטה למדי שלא מניחה כמעט כלום על {% equation %}\gamma{% endequation %} מלבד זה שהיא רציפה. בשביל חישוב האורך נצטרך ש-{% equation %}\gamma{% endequation %} תהיה גם חלקה, אבל בואו קודם נתחיל מהגדרת האורך של העקומה. שימו לב שההגדרה הזו <strong>לא</strong> תחול על כל העקומות, פשוט כי על פיה יוצא שיש עקומות בעלות אורך אינסופי למרות שהן נוצרות מהקטע {% equation %}\left[a,b\right]{% endequation %} בעל האורך הסופי; אלו עקומות "פתולוגיות" חריגות, ואני לא אתעסק איתן; אני כן אגיד שאומרים שעקומה היא Rectifiable (לא יודע איך זה נקרא בעברית) אם ההגדרה שאתן עכשיו עובדת ונותנת אורך סופי.

הרעיון הבסיסי מאחורי ההגדרה הוא ההנחה/אקסיומה/וואטאבר שהמרחק הקצר ביותר בין שתי נקודות הוא הקו הישר שמחבר אותן - זה מה שקורה בגאומטריה האוקלידית, אבל לאו דווקא נכון בגאומטריות אחרות (תלוי מה זה "קו ישר"). אם מקבלים את ההנחה הזו, אז אפשר לחשוב על שיטת <strong>קירוב</strong> לאורך של עקומה שמתבססת על לקחת סדרת נקודות על העקומה, לחבר אותן בקווים ישרים ולקבל <strong>קירוב פוליגוני</strong> של העקומה על ידי משהו שקל לנו יחסית לחשב את האורך שלו כי לחשב אורך של קו זה קל. פורמלית, אם העקומה שלנו היא {% equation %}\gamma:\left[a,b\right]\to\mathbb{R}^{n}{% endequation %}, אז לוקחים חלוקה {% equation %}a=t_{0}<t_{1}<t_{2}<\ldots<t_{m}=b{% endequation %} שאני מסמן ב-{% equation %}P{% endequation %} כמו קודם, ועכשיו אפשר לסמן את הקירוב הפוליגוני ש-{% equation %}P{% endequation %} מגדירה עם {% equation %}\pi\left(P\right){% endequation %} ולהגדיר את האורך שלו בתור

{% equation %}\left|\pi\left(P\right)\right|=\sum_{i=1}^{m}\|\gamma\left(t_{i}\right)-\gamma\left(t_{i-1}\right)\|{% endequation %}

צריך טיפה להסביר מה קורה כאן. ראשית, אם {% equation %}v=\left(v_{1},\ldots,v_{n}\right)\in\mathbb{R}^{n}{% endequation %} אז במטריקה האוקלידית מגדירים את <strong>הנורמה</strong> שלו להיות

{% equation %}\|v\|=\sqrt{\sum_{k=1}^{n}\left|v_{k}\right|^{2}}{% endequation %}

זה קצת מפחיד אבל אם מסתכלים על המקרה של {% equation %}n=2{% endequation %} רואים שהמרחק בין שתי נקודות {% equation %}\left(x_{1},y_{1}\right),\left(x_{2},y_{2}\right){% endequation %} יוצא 

{% equation %}\|\left(x_{1},y_{1}\right)-\left(x_{1},y_{1}\right)\|=\sqrt{\left|x_{1}-x_{2}\right|^{2}+\left|y_{1}-y_{2}\right|^{2}}{% endequation %}

וזה פשוט שימוש רגיל במשפט פיתגורס, אז הנורמה ה-{% equation %}n{% endequation %}-ממדית היא פשוט הכללה של זה. מה שאולי טיפה פחות ברור הוא ש-{% equation %}\gamma\left(t_{i}\right)-\gamma\left(t_{i-1}\right){% endequation %} הוא הנקודה שהקו שמחבר אותה עם ראשית הצירים הוא מאותו אורך כמו הקו שמחבר את {% equation %}\gamma\left(t_{i-1}\right){% endequation %} עם {% equation %}\gamma\left(t_{i}\right){% endequation %} - זה שימוש <strong>בכלל המקבילית</strong> שקל להבין עם איור.

<img src="{{site.baseurl}}{{site.post_images}}/2024/vectors.png" alt=""/>

באיור הזה אני מצייר שתי נקודות {% equation %}a,b{% endequation %}, כשהוקטור אל {% equation %}a{% endequation %} הוא כחול והוקטור אל {% equation %}b{% endequation %} הוא אדום. הרעיון בכלל המקבילית הוא שכאשר אני <strong>מחבר</strong> שני וקטורים, אני מדביק עותק של כל אחד מהם לקצה של השני, כך שאני יוצאר מקבילית, והנקודה שבה שני העותקים הללו נפגשים היא הסכום. אצלנו אני מתעניין ב-{% equation %}a-b{% endequation %} ולכן אני מצייר מקבילית שהוקטורים שבונים אותה הם {% equation %}b{% endequation %} ו-{% equation %}a-b{% endequation %} (שהוקטור שמתאים לו הוא סגול), ואפשר לראות איך אורך הוקטור של {% equation %}a-b{% endequation %} (הקו הסגול התחתון יותר, שמחבר את {% equation %}a-b{% endequation %} עם ראשית הצירים) זהה באורכו לקו הסגול העליון, שהוא מה שמחבר את {% equation %}b{% endequation %} עם {% equation %}a{% endequation %}.

עכשיות אמרתי שהרעיון בקירוב פוליגוני הוא שהקו הישר בין שתי נקודות הוא תמיד <strong>קצר יותר</strong> מאשר העקומה שעוברת דרכן (ליתר דיוק - לא ארוך יותר, כי אולי גם העקומה היא קו ישר בין שתי הנקודות הללו) לכן אנחנו מצפים מאורך העקומה להיות <strong>חסם עליון</strong> עבור כל אורך של קירוב פוליגונלי אליה. מצד שני, ככל שאנחנו מוסיפים יותר ויותר נקודות כך אפשר לקוות שהקירובים שלנו מתקרבים יותר ויותר אל העקומה - כלומר, אנחנו מצפים לכך שהאורך של הקירובים ילך ויתקרב אל אורך העקומה עצמו, בלי "להיתקע" מתחת לאורך קטן יותר בדרך. לכן אנחנו מצפים שאורך העקומה יהיה <strong>החסם העליון הקטן ביותר</strong> של קבוצת אורכי הקירובים הפוליגונליים - או כמו שזה נקרא במתמטיקה, <strong>סופרמום</strong>. לכן אנחנו <strong>מגדירים</strong> את האורך של העקומה {% equation %}\gamma{% endequation %} בין הנקודות {% equation %}a,b{% endequation %} להיות {% equation %}\Lambda_{\gamma}\left(a,b\right)=\sup_{P}\left\{ \left|\pi\left(P\right)\right|\right\} {% endequation %} כאשר {% equation %}P{% endequation %} רץ על כל החלוקות הסופיות של {% equation %}\left[a,b\right]{% endequation %}. בשביל שההגדרה הזו באמת תעבוד, הכרחי שבכלל יהיה חסם סופי לקבוצה הזו, כלומר שיהיה קיים {% equation %}M{% endequation %} כלשהו כך ש-{% equation %}\left|\pi\left(P\right)\right|\le M{% endequation %} לכל חלוקה {% equation %}P{% endequation %}; אם אין כזה, העקום הוא Nonrectifiable.

זה מטפל בהגדרה, אבל מה עם <strong>חישוב</strong> של האורך? בשביל זה אני מכניס לתמונה את ההנחה ש-{% equation %}\gamma{% endequation %} גזירה ברציפות, ואני רוצה להראות שבמקרה הזה יתקיים {% equation %}\Lambda_{\gamma}\left(a,b\right)=\int_{a}^{b}\|\gamma^{\prime}\left(t\right)\|dt{% endequation %}, אבל זה לא יהיה טריוויאלי להראות את זה - למעשה, זה הלב הטכני של הפוסט הזה.

ראשית, אינטואיציות: אם {% equation %}\gamma\left(t\right){% endequation %} היא פונקציה שמתארת <strong>טיול</strong> על העקומה, במובן של "בזמן {% equation %}t{% endequation %} הייתי כאן וכאן", אז הנגזרת שלה, {% equation %}\gamma^{\prime}\left(t\right){% endequation %}, מתארת את <strong>מהירות</strong> הטיול הזה - מהירות במובן שנקרא בפיזיקה Velocity, כלומר כזה שמדבר גם על כיוון התנועה ולא רק על הגודל שלה. אם נסתכל על {% equation %}\|\gamma^{\prime}\left(t\right)\|{% endequation %} נקבל את ה<strong>מהירות</strong> במובן של Speed, גודל בלבד שלא תלוי בכיוון. לכן האינטגרל {% equation %}\int_{a}^{b}\|\gamma^{\prime}\left(t\right)\|dt{% endequation %} מודד "כמה מרחק עברתי". זו נראית הגדרה כל כך מתבקשת שרוב הספרים מתחילים איתה וזהו; רק אני (ולמשל הספר של Tom Apostol שהחלק הזה של הפוסט מסתמך מאוד על דרך ההצגה שלו) מתעקש לדבר על הסופרמום (טוב, אם להודות על האמת, לא מעט ספרים מדברים על הסופרמום ואז אומרים בנפנוף ידיים שזה שווה לאינטגרל וגם זה בסדר).

שנית, למה זה בעצם טריקי? אני אראה עכשיו הוכחה שגויה, שהיא פחות או יותר משהו שהופיע בפוסט המקורי שלי. אנחנו לוקחים קירוב פוליגונלי עם {% equation %}P{% endequation %} כלשהי:

{% equation %}\left|\pi\left(P\right)\right|=\sum_{i=1}^{m}\|\gamma\left(t_{i}\right)-\gamma\left(t_{i-1}\right)\|{% endequation %}

ואז אנחנו אומרים - היי, אנחנו מכירים דרך טובה להתמודד עם ביטויים שהם הפרש של הפונקציה בשתי נקודות שונות; זה מזכיר לנו משפט אולטרה-שימושי מחדו"א, <strong>משפט הערך הממוצע של לגראנז'</strong>. המשפט הזה אומר שאם {% equation %}f:\left[a,b\right]\to\mathbb{R}{% endequation %} היא פונקציה רציפה ובנוסף היא גזירה ב-{% equation %}\left(a,b\right){% endequation %} אז קיימת נקודה {% equation %}c\in\left(a,b\right){% endequation %} כך ש-{% equation %}f^{\prime}\left(c\right)=\frac{f\left(b\right)-f\left(a\right)}{b-a}{% endequation %} - הנגזרת של {% equation %}f{% endequation %} מקבלת ב-{% equation %}c{% endequation %} את <strong>הערך הממוצע</strong> של הפונקציה {% equation %}f{% endequation %} בקטע. זה מאפשר לנו לומר ש-{% equation %}f\left(b\right)-f\left(a\right)=\left(b-a\right)f^{\prime}\left(c\right){% endequation %}, כלומר להמיר את ההפרש בין ערכי הפונקציה בקטע, אל אורך הקטע כפול הנגזרת בתוכו. אז אם אני מפעיל את זה על הביטוי שלמעלה אני מקבל

{% equation %}\sum_{i=1}^{m}\|\gamma\left(t_{i}\right)-\gamma\left(t_{i-1}\right)\|=\sum_{i=1}^{m}\|\gamma^{\prime}\left(t_{i}^{*}\right)\|\Delta t_{i}{% endequation %}

והדבר הזה נראה כמו <strong>סכום רימן</strong> של הפונקציה {% equation %}f\left(t\right)=\|\gamma^{\prime}\left(t\right)\|{% endequation %}, כלומר ככל שהחלוקה {% equation %}P{% endequation %} קטנה יותר כך הקירוב הפוליגונלי שואף גם ל-{% equation %}\Lambda_{\gamma}\left(a,b\right){% endequation %} וגם אל {% equation %}\int_{a}^{b}\|\gamma^{\prime}\left(t\right)\|dt{% endequation %}. זה נראה מצוין, אבל הבעיה היא <strong>שאי אפשר </strong>להשתמש כאן במשפט הערך הממוצע של לגראנז'. כי המשפט הזה עובד עבור פונקציות שהטווח שלהן הוא {% equation %}\mathbb{R}{% endequation %}, ואילו במקרה שלנו {% equation %}\gamma{% endequation %} היא <strong>פונקציה וקטורית</strong>, פונקציה שהטווח שלה הוא {% equation %}\mathbb{R}^{n}{% endequation %}, ובאופן כללי משפט הערך הממוצע לא עובד עבורן. לכן או שצריך עוד עבודה טכנית כדי להציל את ההוכחה הזו, או שצריך ללכת על הוכחה שונה, וזה מה שאני הולך לעשות כאן.

לפני שנעבור להוכחה, בואו נדבר שניה על מה זה בעצם אומר ש-{% equation %}\gamma{% endequation %} היא פונקציה וקטורית, כי עד עכשיו החבאתי את המורכבות של זה בכוונה. בפועל זה אומר שקיימות פונקציות ממשיות {% equation %}\gamma_{1},\gamma_{2},\ldots,\gamma_{n}:\left[a,b\right]\to\mathbb{R}{% endequation %} כך ש-{% equation %}\gamma\left(t\right)=\left(\gamma_{1}\left(t\right),\ldots,\gamma_{n}\left(t\right)\right){% endequation %}. אפשר להוכיח שהרציפות של {% equation %}\gamma{% endequation %} עוברת לפונקציות הרכיבים הללו, וש-{% equation %}\gamma^{\prime}\left(t\right)=\left(\gamma_{1}^{\prime}\left(t\right),\ldots,\gamma_{n}^{\prime}\left(t\right)\right){% endequation %} ובהמשך אני גם אשתמש בסימון {% equation %}\int_{a}^{b}\gamma\left(t\right)dt{% endequation %} כשהכוונה היא לוקטור {% equation %}\left(\int_{a}^{b}\gamma_{1}\left(t\right)dt,\ldots,\int_{a}^{b}\gamma_{n}\left(t\right)dt\right){% endequation %}.

עכשיו בואו נעבור להוכחה. הטריק מאחוריה די מזכיר את האופן שבו מגדירים אינטגרל לא מסוים ומחברים אותו אל האינטגרל המסוים עם המשפט היסודי של החדו"א. אנחנו נגדיר פונקציה ממשית {% equation %}s:\left[a,b\right]\to\mathbb{R}{% endequation %} שמודדת את המרחק שעברנו לאורך העקום מתחילתו ועד הנקודה שהגענו אליה, כלומר {% equation %}s\left(t\right)=\Lambda_{\gamma}\left(a,t\right){% endequation %} (בפרט, {% equation %}s\left(a\right)=0{% endequation %}). אם נצליח להראות ש-{% equation %}s^{\prime}\left(t\right)=\|\gamma^{\prime}\left(t\right)\|{% endequation %} בכל נקודה {% equation %}t\in\left[a,b\right]{% endequation %}, אז נוכל להשתמש במשפט היסודי של החדו"א כדי להראות ש-

{% equation %}\Lambda_{\gamma}\left(a,b\right)=s\left(b\right)-s\left(a\right)=\int_{a}^{b}s^{\prime}\left(t\right)dt=\int_{a}^{b}\|\gamma^{\prime}\left(t\right)\|dt{% endequation %}

אז המטרה שלנו היא להוכיח שמתקיים {% equation %}s^{\prime}\left(t\right)=\|\gamma^{\prime}\left(t\right)\|{% endequation %}. זה יהיה קצת טריקי, ואני אצטרך שתי תוצאות לפני כן:

<ol> <li>{% equation %}\Lambda_{\gamma}\left(a,b\right)\le\int_{a}^{b}\|\gamma^{\prime}\left(t\right)\|dt{% endequation %}, כלומר האינטגרל הוא קירוב מלמעלה של האורך.</li>


<li>פונקציית אורך העקומה היא <strong>חיבורית</strong> ("אדיטיבית") במובן הבא: אם {% equation %}c\in\left[a,b\right]{% endequation %} אז {% equation %}\Lambda_{\gamma}\left(a,b\right)=\Lambda_{\gamma}\left(a,c\right)+\Lambda_{\gamma}\left(c,b\right){% endequation %}</li>

</ol>

נתחיל מ-1. בגלל ש-{% equation %}\Lambda_{\gamma}\left(a,b\right){% endequation %} הוגדר בתור סופרמום על קבוצה, אם נוכיח שכל איבר בקבוצה קטן או שווה למשהו, גם הסופרמום יהיה קטן או שווה ממנו. לכן אנחנו לוקחים חלוקה {% equation %}P{% endequation %} כללית ורוצים להוכיח ש-{% equation %}\left|\pi\left(P\right)\right|\le\int_{a}^{b}\|\gamma^{\prime}\left(t\right)\|dt{% endequation %}. כאן נלך על פי ההגדרות ועל פי תכונות בסיסיות של אינטגרלים:

{% equation %}\left|\pi\left(P\right)\right|=\sum_{i=1}^{m}\|\gamma\left(t_{i}\right)-\gamma\left(t_{i-1}\right)\|=\sum_{i=1}^{m}\|\int_{t_{i-1}}^{t_{i}}\gamma^{\prime}\left(t\right)dt\|\le{% endequation %}

{% equation %}\sum_{i=1}^{m}\int_{t_{i-1}}^{t_{i}}\|\gamma^{\prime}\left(t\right)\|dt=\int_{a}^{b}\|\gamma^{\prime}\left(t\right)\|dt{% endequation %}

בואו נבין את המעברים. הראשון הוא פשוט הגדרת האורך שכבר ראינו. השני הוא שימוש במשפט היסודי של החדו"א. המעבר האחרון משתמש באדיטיביות של אינטגרלים ({% equation %}\int_{a}^{c}f\left(t\right)dt+\int_{c}^{b}f\left(t\right)dt=\int_{a}^{b}f\left(t\right)dt{% endequation %} - דומה למה שאנחנו הולכים להוכיח עבור {% equation %}\Lambda{% endequation %}). המעבר שבו מתחבא הלב הטכני הוא זה שמסתמך על {% equation %}\|\int_{t_{i-1}}^{t_{i}}\gamma^{\prime}\left(t\right)dt\|\le\int_{t_{i-1}}^{t_{i}}\|\gamma^{\prime}\left(t\right)\|dt{% endequation %}. כאן אני מרשה לעצמי סוף סוף לעצור ולא להוכיח את הטענה הזו, פשוט כי היא הכללה טבעית של טענה מוכרת עבור פונקציות ממשיות עם משתנה יחיד (להבדיל ממשפט הערך הממוצע של לגראנז' שפשוט לא היה אפשר להכליל). אם רוצים את ההוכחה, היא נמצאת למשל בספר של Apostol ואולי יום אחד יהיה לי התקף של רצון עז להוכיח גם אותה - אבל הפעם אני נמנע מזה כי זה יאריך את הפוסט הארוך ממילא הזה אפילו עוד יותר.

עכשיו בואו נוכיח את האדיטיביות של {% equation %}\Lambda_{\gamma}\left(a,b\right){% endequation %}. זה יהיה טיעון קליל וחמוד ומאוד חדו"אי באופי שלו. אנחנו לוקחים נקודה כלשהי {% equation %}c\in\left[a,b\right]{% endequation %} ורוצים להוכיח ש--{% equation %}\Lambda_{\gamma}\left(a,b\right)=\Lambda_{\gamma}\left(a,c\right)+\Lambda_{\gamma}\left(c,b\right){% endequation %}, אז יהיה מעורב פה אי שוויון דו כיווני. מכיוון ש-{% equation %}\Lambda_{\gamma}{% endequation %} מוגדר בתור סופרמום על קבוצת איברים, הדרך להתמודד איתו היא על ידי לקיחת איבר כלשהו מהקבוצה ומעבר לרמה הזו של הדיון. 

שימו לב שלפני שנוכיח {% equation %}\Lambda_{\gamma}\left(a,b\right)=\Lambda_{\gamma}\left(a,c\right)+\Lambda_{\gamma}\left(c,b\right){% endequation %} בכלל צריך להוכיח ש-{% equation %}\Lambda_{\gamma}\left(a,c\right),\Lambda_{\gamma}\left(c,b\right){% endequation %} מוגדרים בכלל - שתת-העקומות הללו הן Rectifiable, מה שלא נתון לי כי הנתון מדבר רק על {% equation %}\Lambda_{\gamma}\left(a,b\right){% endequation %}. ובכן, בואו ניקח שתי חלוקות, חלוקה {% equation %}P_{1}{% endequation %} של {% equation %}\left[a,b\right]{% endequation %} וחלוקה {% equation %}P_{2}{% endequation %} של {% equation %}\left[c,b\right]{% endequation %}. האיחוד של שתי החלוקות הללו נותן לי חלוקה {% equation %}P{% endequation %} של {% equation %}\left[a,b\right]{% endequation %} ואם אני מסתכל על הקירוב הפוליגונלי {% equation %}\pi\left(P\right){% endequation %} הוא כולל בדיוק את הקווים שב-{% equation %}\pi\left(P_{1}\right){% endequation %} ו-{% equation %}\pi\left(P_{2}\right){% endequation %} ולכן

{% equation %}\left|\pi\left(P_{1}\right)\right|+\left|\pi\left(P_{2}\right)\right|=\left|\pi\left(P\right)\right|\le\Lambda_{\gamma}\left(a,b\right){% endequation %}

בפרט, קיבלתי שקיים חסם מלעיל עבור {% equation %}\left|\pi\left(P_{1}\right)\right|{% endequation %} וגם עבור {% equation %}\left|\pi\left(P_{2}\right)\right|{% endequation %} ולכן {% equation %}\Lambda_{\gamma}\left(a,c\right),\Lambda_{\gamma}\left(c,b\right){% endequation %} קיימים. אבל כדי להראות את הכיוון הראשון של אי שוויון שאנחנו רוצים, {% equation %}\Lambda_{\gamma}\left(a,c\right)+\Lambda_{\gamma}\left(c,b\right)\le\Lambda_{\gamma}\left(a,b\right){% endequation %}, נצטרך לעבוד עוד טיפה. ניקח את אי השוויון שקיבלנו

{% equation %}\left|\pi\left(P_{1}\right)\right|+\left|\pi\left(P_{2}\right)\right|\le\Lambda_{\gamma}\left(a,b\right){% endequation %}

נעביר אגף ונקבל

{% equation %}\left|\pi\left(P_{1}\right)\right|\le\Lambda_{\gamma}\left(a,b\right)-\left|\pi\left(P_{2}\right)\right|{% endequation %}

אם אני משאיר את {% equation %}P_{2}{% endequation %} קבוע ומרשה ל-{% equation %}P_{1}{% endequation %} לרוץ על כל קבוצת החלוקות של {% equation %}\left[a,b\right]{% endequation %} אנחנו רואים ש-{% equation %}\Lambda_{\gamma}\left(a,b\right)-\left|\pi\left(P_{2}\right)\right|{% endequation %} הוא חסם מלעיל של אורכי כל החלוקות בקבוצה הזו, ולכן הסופרמום של הקבוצה גם כן קטן ממנו. הסופרמום הוא בדיוק {% equation %}\Lambda_{\gamma}\left(a,c\right){% endequation %} אז קיבלנו

{% equation %}\Lambda_{\gamma}\left(a,c\right)\le\Lambda_{\gamma}\left(a,b\right)-\left|\pi\left(P_{2}\right)\right|{% endequation %}

אי השוויון הזה נכון <strong>לכל</strong> חלוקה {% equation %}P_{2}{% endequation %} שניקח, ולכן אפשר להסיק ממנו ש-{% equation %}\Lambda_{\gamma}\left(a,c\right)\le\Lambda_{\gamma}\left(a,b\right)-\Lambda_{\gamma}\left(b,c\right){% endequation %}. אם זה נראה לכם ברור, נהדר! אבל למקרה שלא, בואו נראה את זה פורמלית בכל זאת. אני אשתמש בטכניקה הסטנדרטית: אני אראה שלכל {% equation %}\varepsilon>0{% endequation %} מתקיים {% equation %}\Lambda_{\gamma}\left(a,c\right)\le\Lambda_{\gamma}\left(a,b\right)-\Lambda_{\gamma}\left(b,c\right)+\varepsilon{% endequation %} ומכיוון שזה קורה <strong>לכל</strong> {% equation %}\varepsilon>0{% endequation %} אז אי השוויון חייב להתקיים גם כשאני מציב {% equation %}\varepsilon=0{% endequation %}, כי באופן כללי - אם {% equation %}X\le Y+\varepsilon{% endequation %} לכל {% equation %}\varepsilon>0{% endequation %} אבל {% equation %}X>Y{% endequation %} אז ניקח {% equation %}\varepsilon=\frac{X-Y}{2}{% endequation %} ונקבל 

{% equation %}X\le Y+\varepsilon=Y+\frac{X-Y}{2}=\frac{X+Y}{2}<\frac{X+X}{2}=X{% endequation %}

וקיבלנו {% equation %}X<X{% endequation %}, וזו בוודאי סתירה. אז חזרה למקרה שלנו, לקחנו {% equation %}\varepsilon>0{% endequation %} כלשהו, ועכשיו אנחנו מסתכלים על {% equation %}\Lambda_{\gamma}\left(b,c\right){% endequation %}. מכיוון שזה סופרמום של קבוצה, אז קיים בקבוצה איבר {% equation %}\left|\pi\left(P_{2}\right)\right|{% endequation %} כך ש-{% equation %}\left|\pi\left(P_{2}\right)\right|\ge\Lambda_{\gamma}\left(b,c\right)-\varepsilon{% endequation %}, כלומר {% equation %}-\left|\pi\left(P_{2}\right)\right|\le-\Lambda_{\gamma}\left(b,c\right)+\varepsilon{% endequation %} ולכן

{% equation %}\Lambda_{\gamma}\left(a,c\right)\le\Lambda_{\gamma}\left(a,b\right)-\left|\pi\left(P_{2}\right)\right|\le\Lambda_{\gamma}\left(a,b\right)-\Lambda_{\gamma}\left(b,c\right)+\varepsilon{% endequation %}

כמו שרצינו. זה מראה ש-{% equation %}\Lambda_{\gamma}\left(a,c\right)\le\Lambda_{\gamma}\left(a,b\right)-\Lambda_{\gamma}\left(b,c\right){% endequation %}, ואחרי העברת אגפים {% equation %}\Lambda_{\gamma}\left(a,c\right)+\Lambda_{\gamma}\left(b,c\right)\le\Lambda_{\gamma}\left(a,b\right){% endequation %} וזה הכיוון הראשון של אי השוויון שרצינו.

עבור הכיוון השני אז בואו ניקח חלוקה {% equation %}P{% endequation %} של {% equation %}\left[a,b\right]{% endequation %}. ונראה שלא משנה מה, {% equation %}\left|\pi\left(P\right)\right|\le\Lambda_{\gamma}\left(a,c\right)+\Lambda_{\gamma}\left(c,b\right){% endequation %} ומזה ינבע ש-{% equation %}\Lambda_{\gamma}\left(a,b\right)\le\Lambda_{\gamma}\left(a,c\right)+\Lambda_{\gamma}\left(c,b\right){% endequation %}.

אם אני לוקח את {% equation %}P{% endequation %} ומוסיף לה את הנקודה {% equation %}c{% endequation %} (אלא אם {% equation %}c{% endequation %} כבר נמצאת בה), מה משתנה? אני יכול עכשיו להסתכל על {% equation %}P{% endequation %} בתור איחוד של שתי חלוקות, חלוקה {% equation %}P_{1}{% endequation %} של {% equation %}\left[a,c\right]{% endequation %} וחלוקה {% equation %}P_{2}{% endequation %} של {% equation %}\left[c,b\right]{% endequation %}. החלוקות הללו כוללת בדיוק את אותם קטעים כמו ב-{% equation %}P{% endequation %} למעט אולי קטע {% equation %}\left[t_{i-1},t_{i}\right]{% endequation %} שעבורו {% equation %}c\in\left(t_{i-1},t_{i}\right){% endequation %}, ובמקרה זה הקטע הזה הוחלף בשני הקטעים {% equation %}\left[t_{i-1},c\right]{% endequation %} ו-{% equation %}\left[c,t_{i}\right]{% endequation %}. כעת נכניס לתמונה את אי שוויון המשולש ב-{% equation %}\mathbb{R}^{n}{% endequation %}:

{% equation %}\|\gamma\left(t_{i}\right)-\gamma\left(t_{i-1}\right)\|=\|\gamma\left(t_{i}\right)-\gamma\left(c\right)+\gamma\left(c\right)-\gamma\left(t_{i-1}\right)\|\le\|\gamma\left(t_{i}\right)-\gamma\left(c\right)\|+\|\gamma\left(c\right)-\gamma\left(t_{i-1}\right)\|{% endequation %}

כלומר, אורך הקטע שהסרנו קטן או שווה לאורך שני הקטעים שהוספנו, ולכן נקבל

{% equation %}\left|\pi\left(P\right)\right|\le\left|\pi\left(P_{1}\right)\right|+\left|\pi\left(P_{2}\right)\right|\le\Lambda_{\gamma}\left(a,c\right)+\Lambda_{\gamma}\left(c,b\right){% endequation %}

מה שמסיים את הכיוון הזה, ואת הוכחת האדיטיביות. עכשיו אפשר לחזור אל העיקר: ההוכחה ש-{% equation %}\Lambda_{\gamma}\left(a,b\right)=\int_{a}^{b}\|\gamma^{\prime}\left(t\right)\|dt{% endequation %} שכבר צמצמנו אל הצורך להוכיח רק {% equation %}s^{\prime}\left(t\right)=\|\gamma^{\prime}\left(t\right)\|{% endequation %} כאשר, כזכור, {% equation %}s\left(t\right)=\Lambda_{\gamma}\left(a,t\right){% endequation %}. בשביל לראות איך עושים את זה, בואו נחזור ליסודות - איך מגדירים נגזרת? {% equation %}s^{\prime}\left(t\right)=\lim_{h\to0}\frac{s\left(t+h\right)-s\left(t\right)}{h}{% endequation %}. אני אנסה לחסום את הביטוי הזה משני הכיוונים על ידי גבולות ששואפים לאותו דבר כש-{% equation %}h\to0{% endequation %} ואז להשתמש <strong>בכלל הסנדוויץ'</strong>.

בתור התחלה, בואו נסתכל על {% equation %}\|\gamma^{\prime}\left(t\right)\|{% endequation %}. אם אני מנסה להבין אותו בתור גבול, אני מקבל

{% equation %}\lim_{h\to0}\|\frac{\gamma\left(t+h\right)-\gamma\left(t\right)}{h}\|=\lim_{h\to0}\frac{1}{h}\|\gamma\left(t+h\right)-\gamma\left(t\right)\|{% endequation %}

כאן {% equation %}\|\gamma\left(t+h\right)-\gamma\left(t\right)\|{% endequation %} הוא בעצם האורך של קו ישר שמחבר את שתי הנקודות {% equation %}\gamma\left(t+h\right){% endequation %} ו-{% equation %}\gamma\left(t\right){% endequation %}. כלומר, זה ביטוי שחסום מלמעלה על ידי אורך העקומה {% equation %}\Lambda_{\gamma}\left(t,t+h\right){% endequation %} (אם {% equation %}h<0{% endequation %} אז {% equation %}t+h{% endequation %} בעצם באה קודם ולכן צריך לכתוב {% equation %}\Lambda_{\gamma}\left(t+h,t\right){% endequation %} אבל העיקרון זהה). אבל 

{% equation %}\Lambda_{\gamma}\left(t,t+h\right)=\Lambda_{\gamma}\left(a,t+h\right)-\Lambda_{\gamma}\left(a,t\right)=s\left(t+h\right)-s\left(t\right){% endequation %} 

בזכות האדיטיביות שהוכחנו קודם, כך שאנחנו מקבלים

{% equation %}\|\gamma\left(t+h\right)-\gamma\left(t\right)\|\le s\left(t+h\right)-s\left(h\right){% endequation %}

ולכן

{% equation %}\frac{1}{h}\|\gamma\left(t+h\right)-\gamma\left(t\right)\|\le\frac{s\left(t+h\right)-s\left(t\right)}{h}{% endequation %}

אגף שמאל פה שואף ל-{% equation %}\|\gamma^{\prime}\left(t\right)\|{% endequation %} כאשר {% equation %}h\to0{% endequation %}, אז נשאר רק לחסום מלמעלה את {% equation %}\frac{s\left(t+h\right)-s\left(t\right)}{h}{% endequation %}. בשביל זה אני אסתמך על כך שכבר הוכחתי {% equation %}\Lambda_{\gamma}\left(a,b\right)\le\int_{a}^{b}\|\gamma^{\prime}\left(t\right)\|dt{% endequation %}, כלומר

{% equation %}s\left(t+h\right)-s\left(t\right)=\Lambda_{\gamma}\left(t,t+h\right)\le\int_{t}^{t+h}\|\gamma^{\prime}\left(t\right)\|dt{% endequation %}

עכשיו, שימו לב שאם אני מגדיר פונקציה {% equation %}f:\left[a,b\right]\to\mathbb{R}{% endequation %} על ידי {% equation %}f\left(x\right)=\int_{a}^{x}\|\gamma^{\prime}\left(t\right)\|dt{% endequation %} אז בזכות העובדה ש-{% equation %}\|\gamma^{\prime}\left(t\right)\|{% endequation %} היא פונקציה רציפה (מה שנובע מכך ש-{% equation %}\gamma{% endequation %} חלקה) המשפט היסודי של החדו"א נותן לי ש-{% equation %}f^{\prime}=\|\gamma^{\prime}\left(t\right)\|{% endequation %} בכל {% equation %}\left[a,b\right]{% endequation %} וש-{% equation %}\int_{t}^{t+h}\|\gamma^{\prime}\left(t\right)\|dt=f\left(t+h\right)-f\left(t\right){% endequation %}. כלומר, קיבלתי את החסם

{% equation %}s\left(t+h\right)-s\left(t\right)\le f\left(t+h\right)-f\left(t\right){% endequation %}

וכשנחלק את שניהם ב-{% equation %}h{% endequation %} נקבל

{% equation %}\frac{s\left(t+h\right)-s\left(t\right)}{h}\le\frac{f\left(t+h\right)-f\left(t\right)}{h}{% endequation %}

וכאשר {% equation %}h\to0{% endequation %} אז אגף ימין שואף, על פי הגדרה, אל {% equation %}f^{\prime}\left(t\right)=\|\gamma^{\prime}\left(t\right)\|{% endequation %}, וזה בדיוק מה שרצינו. זה מסיים את ההוכחה: הראינו שאורך העקומה {% equation %}\gamma{% endequation %} הוא בדיוק {% equation %}\int_{a}^{b}\|\gamma^{\prime}\left(t\right)\|dt{% endequation %}, במקרה שבו {% equation %}\gamma{% endequation %} חלקה, מה שמצדיק את ההגדרה של אורך העקומה באמצעות האינטגרל הזה כדי לחסוך את כל ההתעסקות הטכנית שראינו כאן.

<h2>אינטגרל קווי (מסוג ראשון)</h2>

אפשר לחשוב על אינטגרל רימן בתור סכימה של הערכים של {% equation %}f{% endequation %} לאורך הקו הישר שמחבר את {% equation %}a{% endequation %} אל {% equation %}b{% endequation %} ביקום החד-ממדי {% equation %}\mathbb{R}{% endequation %}. הרעיון ב<strong>אינטגרל קווי</strong> הוא להכליל את אותו קונספט של סכימה בדיוק אל ופונקציות שחיים בתוך מרחב גדול יותר - למשל, ב-{% equation %}\mathbb{R}^{n}{% endequation %}, אבל כדי לשמור על החד-ממדיות של אינטגרל רימן, אנחנו לא סוכמים את הערכים של {% equation %}f{% endequation %} בכל המרחב, אלא על תת-מרחב ש"נראה כמו" משהו חד ממדי, או במילים אחרות - על <strong>עקומה</strong>. הסיבה שיש לנו שני סוגים של אינטגרלים קוויים היא שיש שני סוגים של פונקציות שאנחנו רוצים לבצע עליהן אינטגרציה: פונקציה <strong>סקלרית</strong> {% equation %}f:\mathbb{R}^{n}\to\mathbb{R}{% endequation %} שמחזירה מספר ממשי בודד, ופונקציה <strong>וקטורית</strong> {% equation %}F:\mathbb{R}^{n}\to\mathbb{R}^{n}{% endequation %} שמחזירה וקטור מהמרחב שעליו הפונקציה פועלת (אני לא מכיר דרך נפוצה שבה מטפלים בפונקציות שבהן הטווח הוא {% equation %}\mathbb{R}^{m}{% endequation %} כך ש-{% equation %}m\ne n,1{% endequation %}). 

שתי ההנחות הקבועות שלי בהמשך יהיו שהפונקציה {% equation %}f{% endequation %} או {% equation %}F{% endequation %} שאני מבצע לה אינטגרציה היא <strong>רציפה</strong>, ושהעקומה שאני מבצע עליה את האינטגרציה מיוצגת על ידי {% equation %}\gamma{% endequation %} שהיא פונקציה <strong>חלקה</strong> (גזירה ובעלת נגזרת רציפה). בלי אלו ההוכחות שלי לא הולכות לעבוד (ואני לא בטוח אם הן יכולות לעבוד בכלל או שאפשר לתת דוגמאות נגדיות פתולוגיות).

בואו נתחיל מלדבר על המקרה הראשון. אני אסמן ב-{% equation %}\int_{C}fd\gamma{% endequation %} אינטגרל של {% equation %}f{% endequation %} על העקום {% equation %}C{% endequation %}. בפוסט הקודם שלי על אינטגרלים קוויים דיברתי קצת על מה שנדרש מ-{% equation %}\gamma{% endequation %} כדי שהאינטגרל על {% equation %}C{% endequation %} לא יהיה תלוי בפרמטריזציה {% equation %}\gamma{% endequation %} המדויקת, אבל הפעם אני לא אכנס לזה כי זה לא קשור למה שאני רוצה להוכיח. מה אני כן רוצה להוכיח? את השוויון {% equation %}\int_{C}fd\gamma=\int_{a}^{b}f\left(\gamma\left(t\right)\right)\|\gamma^{\prime}\left(t\right)\|dt{% endequation %} שבו בדרך כלל משתמשים כדי <strong>להגדיר</strong> את משמעות הביטוי {% equation %}\int_{C}fd\gamma{% endequation %}. זה אומר שאני צריך להתחיל מלהגדיר את {% equation %}\int_{C}fd\gamma{% endequation %} בדרך אחרת, בתור הכללה טבעית של סכומי רימן.

בואו נתחיל שוב מאינטואיציה. מה בעצם קורה בביטוי {% equation %}\int_{a}^{b}f\left(\gamma\left(t\right)\right)\|\gamma^{\prime}\left(t\right)\|dt{% endequation %}? אם אנחנו לוקחים את הפונקציה הקבועה {% equation %}f\left(x\right)=1{% endequation %}, אנחנו מקבלים בדיוק את האינטגרל שחישב את האורך של {% equation %}\gamma{% endequation %}. אפשר לדמיין את {% equation %}\gamma{% endequation %} כאילו היא מתארת חוט של חומר שמפוזר במרחב, ואת {% equation %}f{% endequation %} כאילו היא מתארת את <strong>צפיפות</strong> החומר בכל נקודה במרחב, ואנחנו רוצים לחשב את כמות החומר הכוללת; אם הצפיפות היא 1 בכל נקודה, הכמות הזו תהיה בדיוק אורך החוט, אבל אנחנו רוצים לטפל בסיטואציה היותר מורכבת של צפיפות משתנה. אפשר גם לחשוב על זה בצורה הרגילה שבה חושבים על אינטגרלים: באינטגרל רגיל, {% equation %}\int_{a}^{b}f\left(t\right)dt{% endequation %}, אנחנו לכאורה לוקחים את הערך {% equation %}f\left(t\right){% endequation %} של הפונקציה בנקודה קונקרטית {% equation %}t{% endequation %}, כופלים באורך של "המרחק מ-{% equation %}t{% endequation %} אל הנקודה הבאה אחריה", אורך שאנחנו מסמנים ב-{% equation %}dt{% endequation %} וחושבים עליו בתור מספר קטן יותר מכל מספר ממשי, וסוכמים את הכל. אז גם ב-{% equation %}\int_{a}^{b}f\left(\gamma\left(t\right)\right)\|\gamma^{\prime}\left(t\right)\|dt{% endequation %} אפשר לחשוב שאנחנו לוקחים את הערך של {% equation %}f{% endequation %} בנקודה קונקרטית - הפעם בנקודה הקונקרטית על העקומה {% equation %}\gamma\left(t\right){% endequation %} שהיא בעצמה הנקודה שמגיעים אליה בטיול על העקומה שמגיע לנקודה הקונקרטית {% equation %}t{% endequation %}, ואז כופלים את ערך הפונקציה הזו ב"מרחק מ-{% equation %}\gamma\left(t\right){% endequation %} אל הנקודה הבאה על {% equation %}\gamma{% endequation %} אחריה". ראינו כבר שמרחק כזה הוא {% equation %}\|\gamma^{\prime}\left(t\right)\|dt{% endequation %}, אבל כל זה היה נפנופי ידיים אינטואיטיביים בלבד; אין כאן משהו פורמלי. לפורמליזם נגיע עכשיו.

את התשתית כבר יש לנו - אנחנו מבינים את הרעיון של לקחת חלוקה {% equation %}P{% endequation %} של {% equation %}\left[a,b\right]{% endequation %} שמורכבת מהנקודות {% equation %}a=t_{0}<t_{1}<t_{2}<\ldots<t_{m}=b{% endequation %} (אני משתמש ב-{% equation %}m{% endequation %} לאינדקס האחרון כי {% equation %}n{% endequation %} תפוס על ידי המימד של המרחב) ולהסתכל על החלוקה שהיא משרה על העקומה {% equation %}\gamma{% endequation %}. עכשיו נמשיך בדיוק כמו עם סכומי רימן הרגילים: בוחרים סדרה {% equation %}t_{1}^{*},\ldots,t_{m}^{*}{% endequation %} של נקודות כך ש-{% equation %}t_{i}^{*}\in\left[t_{i-1},t_{i}\right]{% endequation %} ואז בונים את הסכום {% equation %}S_{P}^{\gamma}=\sum_{i=1}^{m}f\left(\gamma\left(t_{i}^{*}\right)\right)\Delta\gamma_{i}{% endequation %} כאשר {% equation %}f\left(\gamma\left(t_{i}^{*}\right)\right){% endequation %} הוא באופן מובן מאליו הערך של {% equation %}f{% endequation %} על הנקודה בעקומה שמגיעים אליה בזמן {% equation %}t_{i}^{*}{% endequation %}, ו-{% equation %}\Delta\gamma_{i}{% endequation %} הוא המרחק בין {% equation %}\gamma\left(t_{i}\right){% endequation %} ו-{% equation %}\gamma\left(t_{i-1}\right){% endequation %}.

הגענו להגדרה של האינטגרל עצמו: אם קיים מספר {% equation %}I{% endequation %} כך שלכל {% equation %}\varepsilon>0{% endequation %} קיים {% equation %}\delta>0{% endequation %} כך שלכל חלוקה {% equation %}P{% endequation %} עם {% equation %}\lambda\left(P\right)<\delta{% endequation %} מתקיים {% equation %}\left|S_{P}^{\gamma}-I\right|<\varepsilon{% endequation %}, אז אומרים ש-{% equation %}\int_{C}fd\gamma{% endequation %} קיים ו-{% equation %}\int_{C}fd\gamma=I{% endequation %}. זה ממש 1:1 ההגדרה של אינטגרל רימן הרגיל. נשאר רק לקבל עבורה נוסחה.

בואו נסתכל על המרחק {% equation %}\Delta\gamma_{i}{% endequation %} שמופיע בסכום שבנינו. עבדנו <strong>ממש קשה</strong> כדי לקבל נוסחה שימושית למרחק הזה אז בואו נשתמש בה עכשיו: {% equation %}\Delta\gamma_{i}=\int_{t_{i-1}}^{t_{i}}\|\gamma^{\prime}\left(t\right)\|dt{% endequation %}. במבט ראשון הייצוג הזה ל-{% equation %}\Delta\gamma_{i}{% endequation %} נראה לי מעורר חלחלה. בגרסאות המקוריות והכושלות של הפוסט הזה כתבתי {% equation %}\Delta\gamma_{i}=\left|\gamma\left(t_{i}\right)-\gamma\left(t_{i-1}\right)\right|{% endequation %} ואז השתמשתי בלגראז' כדי לקבל {% equation %}\Delta\gamma_{i}=\Delta t_{i}\cdot\left|\gamma^{\prime}\left(c_{i}\right)\right|{% endequation %} וקיבלתי מייד משהו שנראה כמו סכום רימן של {% equation %}\int_{a}^{b}f\left(\gamma\left(t\right)\right)\|\gamma^{\prime}\left(t\right)\|dt{% endequation %}. אלא שכאמור, השימוש הזה בלגראנז' הוא פשוט שגוי. אני לא יכול לעשות אותו, כי {% equation %}\gamma{% endequation %} היא לא פונקציה ממשית אלא פונקציה וקטורית.

העניין הוא ש<strong>קיימת</strong> הכללה של משפט לגראנז' שבה אני כן יכול להשתמש - הכללה עבור <strong>אינטגרלים</strong>. הנה הניסוח המדויק: אם {% equation %}g:\left[a,b\right]\to\mathbb{R}{% endequation %} היא פונקציה רציפה, אז קיימת {% equation %}c\in\left(a,b\right){% endequation %} כך ש-{% equation %}\int_{a}^{b}g\left(t\right)dt=g\left(c\right)\left(b-a\right){% endequation %}. במקרה שלנו, {% equation %}g\left(t\right)=\|\gamma^{\prime}\left(t\right)\|{% endequation %}. זו פונקציה ממשית, כי הנורמה של וקטור היא מספר ממשי בודד. זו פונקציה רציפה כי היא הרכבה של פונקציה רציפה (הנורמה) על פונקציה שהנחתי שהיא רציפה (הנחתי ש-{% equation %}\gamma{% endequation %} חלקה, לכן {% equation %}\gamma^{\prime}{% endequation %} רציפה). לכן אפשר להשתמש במשפט הזה עבור {% equation %}\Delta\gamma_{i}=\int_{t_{i-1}}^{t_{i}}\|\gamma^{\prime}\left(t\right)\|dt{% endequation %} ולקבל שקיים {% equation %}t_{i}^{*}\in\left(t_{i-1},t_{i}\right){% endequation %} כך ש-{% equation %}\Delta\gamma_{i}=\|\gamma^{\prime}\left(t_{i}^{*}\right)\|\Delta t_{i}{% endequation %}, כמו שרציתי.

אינטואיטיבית, סיימנו: אנחנו אומרים "היי, תראו, מצאנו ייצוג ל-{% equation %}S_{P}^{\gamma}{% endequation %} שנראה בדיוק כמו סכום רימן רגיל!" כשהייצוג הזה הוא {% equation %}S_{P}^{\gamma}=\sum_{i=1}^{m}f\left(\gamma\left(t_{i}^{*}\right)\right)\|\gamma^{\prime}\left(t_{i}^{*}\right)\|\Delta t_{i}{% endequation %}, אבל אם רוצים להיות ממש פורמליים צריך להיזהר. למשל, בביטוי של הסכום מופיע {% equation %}t_{i}^{*}{% endequation %} פעמיים - פעם אחת בתוך {% equation %}\gamma^{\prime}\left(t_{i}^{*}\right){% endequation %}, ולשם הוא הגיע בעזרת משפט לגראנז' האינטגרלי שהמציא אותו יש מאין, אבל הוא גם מופיע בתוך {% equation %}f\left(\gamma\left(t_{i}^{*}\right)\right){% endequation %} ולשם הוא הגיע סתם כי בחרנו סדרת נקודות שרירותית לחלוטין, הרבה לפני שבכלל דיברנו על משפט לגראנז' האינטגרלי. בניסוח זהיר צריך להפוך את היוצרות - קודם לקבל את סדרת הנקודות שלגראנז' נותן, ואז לומר "מכיוון שבסכום רימן אנחנו בוחרים נקודות באופן שרירותי אז ניקח את הנקודות שמצאנו קודם". בואו נעשה את זה מסודר טיפ טופ עד הסוף, כי זה הפוסט שבו אני מרכז את כל הקטנוניות שלי.

אני רוצה להוכיח {% equation %}\int_{C}fd\gamma=\int_{a}^{b}f\left(\gamma\left(t\right)\right)\|\gamma^{\prime}\left(t\right)\|dt{% endequation %}. הדרך החדו"אית הפורמלית להוכיח ששני דברים הם שווים היא להוכיח שלכל {% equation %}\varepsilon>0{% endequation %} מתקיים

{% equation %}\left|\int_{C}fd\gamma-\int_{a}^{b}f\left(\gamma\left(t\right)\right)\|\gamma^{\prime}\left(t\right)\|dt\right|<\varepsilon{% endequation %}

טריק חדו"אי ידוע בשביל להוכיח דבר כזה הוא למצוא מספר {% equation %}S{% endequation %} שקרוב לשני הביטויים הללו עד כדי {% equation %}\frac{\varepsilon}{2}{% endequation %}:

{% equation %}\left|\int_{C}fd\gamma-S\right|<\frac{\varepsilon}{2}{% endequation %}

{% equation %}\left|S-\int_{a}^{b}f\left(\gamma\left(t\right)\right)\|\gamma^{\prime}\left(t\right)\|dt\right|<\frac{\varepsilon}{2}{% endequation %}

אם נמצא מספר כזה, נוכל ללכת לביטוי המקורי, לחבר ולחסר בו את {% equation %}S{% endequation %} ולהשתמש באי שוויון המשולש:

{% equation %}\left|\int_{C}fd\gamma-\int_{a}^{b}f\left(\gamma\left(t\right)\right)\|\gamma^{\prime}\left(t\right)\|dt\right|=\left|\left(\int_{C}fd\gamma-S\right)+\left(S-\int_{a}^{b}f\left(\gamma\left(t\right)\right)\|\gamma^{\prime}\left(t\right)\|dt\right)\right|\le{% endequation %}

{% equation %}\left|\int_{C}fd\gamma-S\right|+\left|S-\int_{a}^{b}f\left(\gamma\left(t\right)\right)\|\gamma^{\prime}\left(t\right)\|dt\right|\le\frac{\varepsilon}{2}+\frac{\varepsilon}{2}=\varepsilon{% endequation %}

מי ה-{% equation %}S{% endequation %} הזה יהיה? כמובן, הוא יהיה <strong>סכום רימן</strong> שקרוב מספיק לשני הביטויים הללו. בואו נבנה אותו בזהירות כדי שסדרת ה-{% equation %}t_{i}^{*}{% endequation %}-ים תתקבל בצורה נכונה.

ראשית, אנחנו יודעים שעבור {% equation %}\frac{\varepsilon}{2}{% endequation %} קיים {% equation %}\delta_{1}>0{% endequation %} כך שלכל חלוקה {% equation %}P{% endequation %} עם {% equation %}\lambda\left(P\right)<\delta_{1}{% endequation %}, <strong>לכל</strong> סכום רימן {% equation %}S_{P}^{\gamma}{% endequation %} שנבנה על החלוקה {% equation %}P{% endequation %} עם בחירה של סדרת נקודות <strong>כלשהי</strong>, מתקיים {% equation %}\left|\int_{C}fd\gamma-S_{P}^{\gamma}\right|<\frac{\varepsilon}{2}{% endequation %}. אני עדיין לא מגדיר את {% equation %}S{% endequation %} המדובר; בינתיים רק קיבלתי את {% equation %}\delta_{1}{% endequation %}.

בנוסף, שעבור {% equation %}\frac{\varepsilon}{2}{% endequation %} קיים {% equation %}\delta_{2}>0{% endequation %} כך שלכל חלוקה {% equation %}P{% endequation %} עם {% equation %}\lambda\left(P\right)<\delta_{2}{% endequation %}, <strong>לכל</strong> סכום רימן {% equation %}S_{P}{% endequation %} שנבנה על החלוקה {% equation %}P{% endequation %} עם בחירה של סדרת נקודות <strong>כלשהי</strong>, מתקיים {% equation %}\left|S_{P}-\int_{a}^{b}f\left(\gamma\left(t\right)\right)\|\gamma^{\prime}\left(t\right)\|dt\right|<\frac{\varepsilon}{2}{% endequation %}. גם פה: עוד לא בניתי את {% equation %}S{% endequation %}, רק מצאתי את {% equation %}\delta_{2}{% endequation %}.

עכשיו אני אגדיר {% equation %}\delta=\min\left\{ \delta_{1},\delta_{2}\right\} {% endequation %}. ואקח חלוקה <strong>כלשהי</strong> {% equation %}P{% endequation %} של {% equation %}\left[a,b\right]{% endequation %} כך ש-{% equation %}\lambda\left(P\right)<\delta{% endequation %} (זו יכולה להיות חלוקה אחידה, למשל; זה לא ממש משנה לי). שימו לב שגם בשלב הזה עדיין לא בניתי את {% equation %}S{% endequation %}; אבל אני כבר יודע שכל סכום רימן שייבנה על פי {% equation %}P{% endequation %} הולך להיות קרוב לאינטגרלים שלעיל. העניין הוא שאני צריך למצוא {% equation %}S{% endequation %} ספציפי כך ש-{% equation %}S=S_{P}=S_{P}^{\gamma}{% endequation %} למרות ש-{% equation %}S_{P}{% endequation %} ו-{% equation %}S_{P}^{\gamma}{% endequation %} מוגדרים בצורה שונה - זה בדיוק האופן שבו לגראנז' נכנס לעניין.

אם כן, אני מפעיל את לגראנז' על החלוקה {% equation %}P{% endequation %} ומוצא סדרת נקודות {% equation %}t_{1}^{*},t_{2}^{*},\ldots,t_{m}^{*}{% endequation %} כך ש-{% equation %}\Delta\gamma_{i}=\|\gamma^{\prime}\left(t_{i}^{*}\right)\|\Delta t_{i}{% endequation %}, ועכשיו אני מגדיר:

{% equation %}S=\sum_{i=1}^{m}f\left(\gamma\left(t_{i}^{*}\right)\right)\|\gamma^{\prime}\left(t_{i}^{*}\right)\|\Delta t_{i}{% endequation %}

עכשיו שיחקתי אותה, כי מצד אחד אם אני אסתכל על סכום הרימן שנוצר על ידי הפונקציה {% equation %}f\left(\gamma\left(t\right)\right)\|\gamma^{\prime}\left(t\right)\|{% endequation %}, החלוקה {% equation %}P{% endequation %} וסדרת הנקודות {% equation %}t_{1}^{*},t_{2}^{*},\ldots,t_{m}^{*}{% endequation %}, הסכום הזה הוא בדיוק

{% equation %}S_{P}=\sum_{i=1}^{m}f\left(\gamma\left(t_{i}^{*}\right)\right)\|\gamma^{\prime}\left(t_{i}^{*}\right)\|\Delta t_{i}=S{% endequation %}

ומצד שני אם אני אסתכל על סכום הרימן {% equation %}S_{P}^{\gamma}{% endequation %} שנוצר על ידי העקומה {% equation %}\gamma{% endequation %}, החלוקה {% equation %}P{% endequation %}, סדרת הנקודות {% equation %}t_{1}^{*},t_{2}^{*},\ldots,t_{m}^{*}{% endequation %} והפונקציה {% equation %}f{% endequation %}, הסכום הזה הוא בדיוק

{% equation %}S_{P}^{\gamma}=\sum_{i=1}^{m}f\left(\gamma\left(t_{i}^{*}\right)\right)\Delta\gamma_{i}=\sum_{i=1}^{m}f\left(\gamma\left(t_{i}^{*}\right)\right)\|\gamma^{\prime}\left(t_{i}^{*}\right)\|\Delta t_{i}=S{% endequation %}

וקיבלתי את {% equation %}S{% endequation %} המבוקש שלי, מה שמסיים את ההוכחה: ראינו ש-{% equation %}\int_{C}fd\gamma=\int_{a}^{b}f\left(\gamma\left(t\right)\right)\|\gamma^{\prime}\left(t\right)\|dt{% endequation %}. מבחינתי הסיפור של הוכחת הנוסחה הזו סגור ואני עכשיו בסדר עם ספרים שמשתמשים בה פשוט בתור ההגדרה.

<h2>אינטגרל קווי (מסוג שני)</h2>

אינטגרל קווי מסוג שני מטפל בסיטואציה שבה מבצעים אינטגרל על פונקציה וקטורית, {% equation %}F:\mathbb{R}^{n}\to\mathbb{R}^{n}{% endequation %}. היה אפשר לחשוב על כל מני דרכים לעשות את זה: אפשר למשל להתייחס אל {% equation %}F{% endequation %} בתור {% equation %}F\left(x\right)=\left(f_{1}\left(x\right),\ldots,f_{n}\left(x\right)\right){% endequation %} כשכל {% equation %}f_{i}{% endequation %} היא פונקציה סקלרית ובמקרה הזה כבר טיפלנו עם אינטגרל קווי מסוג ראשון. בצורה הזו היינו מקבלים אינטגרל שהתוצאה שלו היא וקטור. אני לא רואה משהו שמונע מאיתנו להגדיר דבר כזה, כמו שאני לא רואה סיבה לא להגדיר כפל מטריצות "איבר-איבר". זו פשוט לא הגדרה שימושית במיוחד.

מה כן הגדרה שימושית? או, אם אמרנו <strong>שימושים</strong> אז כאן אין מנוס מלהיכנס <strong>לפיזיקה</strong> כי אינטגרל קווי מסוג שני משמש שם לתיאור אחד מהדברים הבסיסיים ביותר במכניקה - תיאור של <strong>עבודה</strong>. 

בואו נחשוב על הסיטואציה הבאה - אנחנו לוקחים כדורגל ומעיפים אותו גבוה לאוויר, בקו אנכי לגמרי. מה שיקרה הוא שהכדורגל יתחיל לעוף במהירות מסוימת, וככל שכח הכובד יפעל עליו כך המהירות תקטן עוד ועוד עד אשר הכדורגל ייעצר לרגע באוויר, ואז יתחיל לצבור מהירות לכיוון ההפוך. עד שיפול בחזרה למטה. בכל הזמן הזה פעל על הכדור כוח אחד ויחיד - כוח הכובד. בהתחלה הוא הקטין את מהירות הכדור, ואחר כך הוא הגדיל אותה. מה השתנה? דרך אחת לחשוב על כך היא זו: בהתחלה הכדור זז <strong>למעלה</strong> בזמן שכוח הכובד פעל <strong>למטה</strong>, ואחר כך הכדור זז <strong>למטה</strong> תוך כדי שכוח הכובד פועל <strong>למטה</strong>. יש כאן קשר בין וקטור הכוח (כלומר לא רק הגודל שלו, גם הכיוון שלו) ומסלול התנועה של הכדור.

הנה עוד סיטואציה לדוגמא: נדמיין לווין שמסתובב סביב כדור הארץ במסלול מעגלי לגמרי. מסלול מעגלי שכזה לא מתרחש "מעצמו"; החוק הראשון של ניוטון אומר שאם לא מופעלים על גוף כוחות, הוא יתמיד במסלול שהוא קו ישר. מסלול מעגלי נוצר רק כשבכל רגע משנים את כיוון התנועה של האובייקט. כאן ספציפית אפשר לדמיין שהלווין נע <strong>שמאלה</strong> בזמן שכדור הארץ מפעיל עליו כוח <strong>למטה</strong>, בניצב לכיוון התנועה של הלווין. זה גורם לכיוון התנועה של הלווין להשתנות ולהיות "שמאלה וקצת למטה" ; בשלב הזה הוא כבר זז קצת אבל כדור הארץ ממשיך להפעיל עליו כוח שניצב לכיוון התנועה שלו, וכן הלאה. בסיטואציה כזו של תנועה מעגלית מושלמת (ולא, נאמר, שהלווין נע במעין אליפסה) הגודל של המהירות של הלווין (מה שנקרא speed, להבדיל מ-velocity) הולך להישאר קבוע - זה שונה מהכדורגל שבו המהירות השתנתה כל הזמן. מה ההבדל? בסיפור של הלווין הכוח פועל <strong>בניצב</strong> לכיוון התנועה של הלווין, ובסיפור הכדורגל וקטור הכוח היה חופף לכיוון התנועה (או שהוא היה זהה לו, או שהוא היה הפוך בכיוונו).

בואו נעבור לפורמליזם הפיזיקלי. בפיזיקה משתמשים ב-{% equation %}v\left(t\right){% endequation %} כדי לתאר את וקטור המהירות של גוף בזמן {% equation %}t{% endequation %}. אם אנחנו במרחב תלת ממדי, למשל, אז {% equation %}v\left(t\right)=\left(v_{x}\left(t\right),v_{y}\left(t\right),v_{z}\left(t\right)\right){% endequation %}. המהירות במובן של speed של הוקטור הזה היא {% equation %}\|v\|=\sqrt{v_{x}^{2}+v_{y}^{2}+v_{z}^{2}}{% endequation %}, אבל אפשר לפשט את הסימונים אם מכניסים לתמונה <strong>מכפלה סקלרית</strong>. באופן כללי, מכפלה סקלרית של שני וקטורים {% equation %}a,b\in\mathbb{R}^{n}{% endequation %} היא {% equation %}a\cdot b=\sum_{i=1}^{n}a_{i}b_{i}{% endequation %}, ולא קשה לראות ש-{% equation %}\|v\|^{2}=v\cdot v{% endequation %} (למי שזוכרים אלגברה לינארית, מכפלה סקלרית היא מקרה פרטי של מכפלה פנימית).

עכשיו, בפיזיקה יש לנו את <strong>החוק השני של ניוטון</strong> שמתאר את האופן שבו כוח שפועל על גוף משפיע על המהירות שלו: {% equation %}F=ma{% endequation %}, כאשר {% equation %}F{% endequation %} הואה כוח שפועל על הגוף ו-{% equation %}a{% endequation %} היא <strong>התאוצה</strong> של הגוף, כלומר {% equation %}a=v^{\prime}{% endequation %} (הפיזיקאים מעדיפים סימון כמו {% equation %}a=\frac{dv}{dt}{% endequation %} ועושים איתו להטוטים אבל אני בכוונה אמנע מכך כאן). עכשיו, מערכת שכוללת גוף וכוח שפועל עליו יכולה להיות מסובכת למדי: הכוח משפיע על התאוצה, שהיא הנגזרת הראשונה של המהירות, ולכן הנגזרת השנייה של המיקום, אבל המיקום עצמו עשוי להשפיע על הכוח כי באופן כללי הכוח תלוי במיקום האובייקט במרחב. אם נפתח את זה עד הסוף נקבל <strong>משוואה דיפרנצאלית</strong> וזה יכול להיות אתגר להתמודד עם דבר כזה, אז הפיזיקאים מוצאים דרכים להתמודד עם הקשיים בלי ללכת איתם ראש בראש, ואחת מהדרכים הללו היא לדבר על <strong>אנרגיה</strong>.

אנרגיה היא גודל מספרי כלשהו שניתן לחשב עבור מערכת, והרעיון בו הוא שהחישוב הוא כזה שהערך של האנרגיה <strong>נשאר קבוע</strong> גם כשהמערכת עוברת שינויים (הרעיון הזה של <strong>אינוריאנטה</strong> ככלי להבנה של מערכות מסובכות הוא להיט גם במתמטיקה; <a href="https://gadial.net/2007/04/23/invariants_15_game_domino/">הפוסט הראשון בבלוג</a> דיבר על זה). אחד מהגדלים שצריך לחשב כדי לקבל את האנרגיה של מערכת הוא <strong>האנרגיה הקינטית</strong> של העצמים שנמצאים בה, שמתארת גודל שנובע מהמהירות שלהם. עבור גוף בעל מסה {% equation %}m{% endequation %} ומהירות {% equation %}v{% endequation %}, האנרגיה הקינטית היא {% equation %}\frac{m\|v\|^{2}}{2}=\frac{m\left(v\cdot v\right)}{2}{% endequation %}. עכשיו, לפני שאתקדם, הנה להטוט חמוד: אם {% equation %}a\left(t\right),b\left(t\right){% endequation %} הן שתי פונקציות וקטוריות, {% equation %}a,b:\mathbb{R}\to\mathbb{R}^{n}{% endequation %}, אז לא קשה להראות בעזרת חוקי הנגזרות הרגילים שמתקיים

{% equation %}\left(a\cdot b\right)^{\prime}=\left(\sum_{i=1}^{n}a_{i}b_{i}\right)^{\prime}=\sum_{i=1}^{n}a_{i}^{\prime}b_{i}+\sum_{i=1}^{n}a_{i}b_{i}^{\prime}=a^{\prime}\cdot b+a\cdot b^{\prime}{% endequation %}

לכן, אם אני אסמן {% equation %}T=\frac{m\|v\|^{2}}{2}{% endequation %} כדי לתאר את האנרגיה הקינטית של גוף, ואז אחשב את קצב השינוי שלה, אני אקבל

{% equation %}T^{\prime}=\frac{m}{2}\left(v\cdot v\right)^{\prime}=mv^{\prime}\cdot v=F\cdot v{% endequation %}

כלומר, השינוי באנרגיה הקינטית של הגוף הוא הכוח {% equation %}F{% endequation %} שפועל עליו, כפול וקטור המהירות של הגוף - זה תואם את הדיון שלמעלה, ומן הסתם לא במקרה - ההגדרה של אנרגיה קינטית מיועדת כדי שזה יעבוד. עכשיו, נפנוף הידיים הפיזיקאי אומר בשלב הזה ש-{% equation %}F\cdot v{% endequation %} מתאר את השינוי <strong>הרגעי</strong> באנרגיה בהתאם לשינוי הרגעי בזמן, ולכן {% equation %}\int_{a}^{b}F\cdot vdt{% endequation %} הולך לתאר את השינוי באנרגיה לאורך פרק הזמן {% equation %}a\le t\le b{% endequation %}, מה שנקרא <strong>העבודה</strong> של הכוח על הגוף. עכשיו, אם נתאר ב-{% equation %}\gamma{% endequation %} את המסלול שהעצם עבר בפרק הזמן הזה, אז {% equation %}v=\gamma^{\prime}{% endequation %} (כי מהירות היא תמיד הנגזרת של המקום), ולכן השינוי באנרגיה של הגוף יהיה {% equation %}\int_{a}^{b}F\cdot\gamma^{\prime}dt{% endequation %}. עכשיו אפשר להחזיר את מה שהסתרנו - הרי {% equation %}F{% endequation %} היא פונקציה שתלויה לא בזמן אלא <strong>במקום</strong> של העצם בכל רגע נתון, כלומר ב-{% equation %}\gamma\left(t\right){% endequation %}, אז את האינטגרל אפשר לכתוב בתור

{% equation %}\int_{a}^{b}F\left(\gamma\left(t\right)\right)\cdot\gamma^{\prime}dt{% endequation %}

הדבר <strong>הזה</strong> הוא איך שמוגדר אינטגרל קווי מסוג שני. לפעמים הוא מסומן גם בתור {% equation %}\int_{C}F\cdot\gamma d\gamma{% endequation %}, בדומה למה שקורה לאינטגרל קווי מסוג ראשון, אבל צריך לזכור שכאן הכפל בין {% equation %}F{% endequation %} ל-{% equation %}\gamma{% endequation %} הוא <strong>מכפלה סקלרית</strong> וחישוב האינטגרל בפועל מסתמך עליה: {% equation %}\int_{C}F\cdot\gamma d\gamma=\int_{a}^{b}F\left(\gamma\left(t\right)\right)\cdot\gamma^{\prime}dt{% endequation %}.

ושוב עולה אצלי השאלה - האם אני <strong>חייב</strong> פשוט להגדיר את האינטגרל להיות {% equation %}\int_{a}^{b}F\left(\gamma\left(t\right)\right)\cdot\gamma^{\prime}dt{% endequation %}? או שאני יכול להשתמש בגישת סכומי רימן גם כאן? בואו נחשוב איך סכום רימן כזה הולך להיראות כאן. כרגיל, אני אקח את הקטע {% equation %}\left[a,b\right]{% endequation %} ואחלק אותו לחלוקה {% equation %}P{% endequation %} כלשהי עם נקודות הביניים {% equation %}a=t_{0}\le t_{1}\le\ldots\le t_{m}=b{% endequation %}. אני אקח נקודה {% equation %}t_{i}^{*}\in\left[t_{i-1},t_{i}\right]{% endequation %} מתוך כל קטע כזה, אחשב את הפונקציה באותה נקודה של העקום שמתאימה לזמן {% equation %}t_{i}^{*}{% endequation %} כלומר אסתכל על {% equation %}F\left(\gamma\left(t_{i}^{*}\right)\right){% endequation %}, ואת זה אני אכפול באורך... לא, רגע, עוד לא. כאמור, {% equation %}F\left(\gamma\left(t_{i}^{*}\right)\right){% endequation %} הוא וקטור שאנחנו לא רוצים לקחת את כולו; אנחנו רוצים לקחת רק את <strong>הגודל של ההיטל</strong> שלו על הכיוון שאליו העקום {% equation %}\gamma{% endequation %} הולך בזמן {% equation %}t_{i}^{*}{% endequation %}. בשביל דברים כאלו יש לנו מכפלה סקלרית.

הנה עוד תזכורת על מכפלה סקלרית. מצד אחד, {% equation %}u\cdot v=\sum_{i=1}^{n}u_{i}v_{i}{% endequation %} וזו דרך לחשוב על מכפלה סקלרית בתור איך בדיוק מחשבים אותה. מצד שני, אפשר להראות ש-{% equation %}u\cdot v=\|u\|\cdot\|v\|\cdot\cos\theta{% endequation %} כש-{% equation %}\theta{% endequation %} היא הזווית שבין שני הוקטורים. עכשיו, אפשר לחשוב על {% equation %}\|u\|\cdot\cos\theta{% endequation %} בתור <strong>גודל ההיטל</strong> של {% equation %}u{% endequation %} על הציר ש-{% equation %}v{% endequation %} מגדיר - הנה דרך לדמיין את זה (החלק של {% equation %}v{% endequation %} עד הקו המקווקו, שצבעתי בסגול, הוא מאורך {% equation %}\|u\|\cdot\cos\theta{% endequation %}):

<img src="{{site.baseurl}}{{site.post_images}}/2024/projection.png" alt=""/>

ולכן אפשר לחשוב על {% equation %}u\cdot v{% endequation %} בתור גודל ההיטל של {% equation %}u{% endequation %} על {% equation %}v{% endequation %}, כל זה כפול הגודל של {% equation %}v{% endequation %}. אם אנחנו רוצים להשתמש ב-{% equation %}v{% endequation %} רק בתור וקטור שמצביע על כיוון, בלי לכפול בגודל שלו, אפשר פשוט לנרמל אותו - להסתכל על המכפלה {% equation %}u\cdot\frac{v}{\|v\|}{% endequation %}. אפשר לעשות את זה גם כאן: אם אנחנו רוצים רק את וקטור הכיוון שאליו {% equation %}\gamma{% endequation %} הולכת בזמן {% equation %}t_{i}^{*}{% endequation %} אפשר להסתכל על הוקטור {% equation %}\frac{\gamma^{\prime}\left(t_{i}^{*}\right)}{\|\gamma^{\prime}\left(t_{i}^{*}\right)\|}{% endequation %}. כמובן, זה מניח ש-{% equation %}\gamma^{\prime}\left(t_{i}^{*}\right)\ne0{% endequation %}, כי אם {% equation %}\gamma^{\prime}\left(t_{i}^{*}\right)=0{% endequation %} אין מה לחלק בנורמה שלו אבל יותר גרוע מזה, הוא בכלל לא מגדיר כיוון מוגדר ולכן כל הדיון חסר תוחלת; אבל אנחנו מניחים שהפרמטריזציה היא "נחמדה" ולכן אין לה סיבה לבצע עצירות פתאומיות, אז נתעלם מזה באלגנטיות.

אם כן, הפונקציה שאנחנו רוצים שתופיע לנו בסכום הרימן, ותוכפל כרגיל ב-{% equation %}\Delta\gamma_{i}{% endequation %} כמו שקרה באינטגרל קווי מסוג ראשון, היא הפונקציה {% equation %}\frac{F\left(\gamma\left(t_{i}^{*}\right)\right)\cdot\gamma^{\prime}\left(t_{i}^{*}\right)}{\|\gamma^{\prime}\left(t_{i}^{*}\right)\|}{% endequation %}, ולכן סכום הרימן שלי הולך להיות

{% equation %}S_{P}^{\gamma}=\sum_{i=1}^{m}\frac{F\left(\gamma\left(t_{i}^{*}\right)\right)\cdot\gamma^{\prime}\left(t_{i}^{*}\right)}{\|\gamma^{\prime}\left(t_{i}^{*}\right)\|}\cdot\Delta\gamma_{i}{% endequation %}

ובכן, זו נראית כמו חתיכת מהומה ענקית! אבל בואו ניזכר שעשינו לא מעט עבודה כשדיברנו על אינטגרל קווי מסוג ראשון כדי להראות שאפשר לכתוב {% equation %}\Delta\gamma_{i}=\|\gamma^{\prime}\left(t_{i}^{*}\right)\|\Delta t_{i}{% endequation %}. זה <strong>לא</strong> נכון לכל סדרה שרירותית של נקודות {% equation %}t_{i}^{*}{% endequation %}! אבל בהינתן חלוקה {% equation %}P{% endequation %} תמיד אפשר למצוא סדרה ספציפית כזו של נקודות שעבורן השוויון יתקיים - זה היה שימוש במשפט לגראנז' האינטגרלי. לכן, עבור בחירה מתאימה של נקודות כאלו, סכום הרימן שלנו הופך להיות

{% equation %}S_{P}^{\gamma}=\sum_{i=1}^{m}F\left(\gamma\left(t_{i}^{*}\right)\right)\cdot\gamma^{\prime}\left(t_{i}^{*}\right)\cdot\Delta t_{i}{% endequation %}

ואפשר להשתמש בדיוק באותה הוכחה שראינו במקרה של אינטגרל קווי מסוג ראשון כדי להראות שבגלל שהסכום הזה הוא בעצם אותו דבר כמו סכום רימן של האינטגרל הרגיל {% equation %}\int_{a}^{b}F\left(\gamma\left(t\right)\right)\cdot\gamma^{\prime}dt{% endequation %}, אנחנו מקבלים {% equation %}\int_{C}F\cdot\gamma d\gamma=\int_{a}^{b}F\left(\gamma\left(t\right)\right)\cdot\gamma^{\prime}dt{% endequation %}. למעשה, אפשר ממש לחשוב על מה שעשינו פה בתור <strong>רדוקציה</strong> למקרה של אינטגרל קווי מסוג ראשון - כאילו אמרנו "היי, בואו נסתכל על הפונקציה {% equation %}\frac{F\left(\gamma\left(t\right)\right)\cdot\gamma^{\prime}\left(t\right)}{\|\gamma^{\prime}\left(t\right)\|}{% endequation %}, זו פונקציה סקלרית אז בואו נחשב לה אינטגרל קווי מסוג <strong>ראשון</strong>". זה מסיים עבורי את הסיפור של אינטגרל קווי מסוג שני - גם פה, אני עכשיו בסדר גמור עם פשוט להגדיר אותו בתור {% equation %}\int_{a}^{b}F\left(\gamma\left(t\right)\right)\cdot\gamma^{\prime}dt{% endequation %}.

<h2>ומה עם אינטגרל מרוכב?</h2>

כל המהומה הזו נולדה מהנסיון לשכנע את עצמי שההגדרה של אינטגרל מרוכב היא "מה שהיא צריכה להיות" למרות שאין שום צורך בשכנוע כזה מלכתחילה כי מרגע שמתחילים עם ההגדרה הזו קורים קסמים. אז אני לא יכול לסיים את הפוסט הזה בלי לדבר גם על ההגדרה הזו.

ההגדרה עצמה דומה מאוד להגדרה של אינטגרל קווי מסוג שני: יש לנו עקומה {% equation %}C{% endequation %}, רק שהפעם היא לא עקומה ב-{% equation %}\mathbb{R}^{n}{% endequation %} אלא <strong>במישור המרוכב</strong>, כלומר אני מתאר אותה עם פונקציה שאסמן {% equation %}z:\left[a,b\right]\to\mathbb{C}{% endequation %}. יש לנו פונקציה מרוכבת {% equation %}f:\mathbb{C}\to\mathbb{C}{% endequation %}, ואנחנו מגדירים 

{% equation %}\int_{C}f\left(z\right)dz=\int_{a}^{b}f\left(z\left(t\right)\right)z^{\prime}\left(t\right)dt{% endequation %}

זה מאוד, מאוד מזכיר אינטגרל קווי מסוג שני, אבל יש הבדל ברור אחד - הכפל שמופיע בתור האינטגרל הימני הוא <strong>לא</strong> מכפלה סקלרית, הוא פשוט פעולת הכפל הרגילה של מספרים מרוכבים, שהיא די שונה ממכפלה סקלרית. אם אני כותב את המספר המרוכב {% equation %}a+bi{% endequation %} בתור {% equation %}\left(a,b\right){% endequation %}, אז נקבל את המכפלה {% equation %}\left(a_{1},b_{1}\right)\cdot\left(a_{2},b_{2}\right)=\left(a_{1}a_{2}-b_{1}b_{2},a_{1}b_{2}+a_{2}b_{1}\right){% endequation %} שהיא כמובן גם לא סקלר ממשי אלא עדיין משהו עם שני רכיבים ממשיים שונים, וגם היא ערבוביה מוחלטת של המקדמים, בזכות פעולת הכפל המוזרה של מספרים מרוכבים. אז אי אפשר להגיד שזה פשוט לקחת את ההגדרה של אינטגרל קווי מסוג שני ולהשתמש בה על מרוכבים; ועוד דבר שהפריע לי מאוד הוא למה להשתמש דווקא באינטגרל קווי מסוג שני ולא באינטגרל קווי מסוג <strong>ראשון</strong>, שלכאורה מתאים יותר לסיטואציה של פונקציה שמחזירה סקלר שאפשר לכפול בו כפל רגיל. למה לא להגדיר {% equation %}\int_{C}f\left(z\right)dz=\int_{a}^{b}f\left(z\left(t\right)\right)\|z^{\prime}\left(t\right)\|dt{% endequation %}?

ובכן, אפשר היה להגדיר ככה, זה פשוט לא היה שימושי כמו מה שכן הוגדר. הטעות הבסיסית שלי כשניגשתי לנושא הייתה לחשוב שאינטגרל מרוכב מוגדר בצורה דומה לאינטגרל קווי, כשבפועל הוא לא: הוא מוגדר בצורה דומה למה שנקרא <strong>אינטגרל רימן-סטילטיס</strong>.

מה הרעיון של אינטגרל רימן-סטילטיס? זה כמובן ראוי לפוסט משלו, אז אשאר כאן יחסית ממוקד. באינטגרל רימן, סכום רימן נראה כמו {% equation %}S_{P}=\sum_{i=1}^{n}f\left(t_{i}^{*}\right)\Delta t_{i}{% endequation %} כאשר {% equation %}\Delta t_{i}=t_{i}-t_{i-1}{% endequation %} והרעיון הוא ש-{% equation %}\Delta t_{i}{% endequation %} מייצג <strong>אורך קטע</strong>. בעצם, אפשר לחשוב על הסכום הזה בתור סכום <strong>ממושקל</strong>, כשהערך של פונקציה בקטע מסוים מקבלת משקל שמתאים לאורך שלו. זה תואם את האינטואיציה שלנו כשאנו חושבים על אינטגרל בתור "השטח שמתחת לגרף הפונקציה {% equation %}f{% endequation %}". אבל למה להגביל את עצמנו? אפשר להשתמש בפונקציות משקל שונות ומשונות שנקראות <strong>אינטגרטורים</strong>. אם כן, לוקחים פונקציה {% equation %}g:\left[a,b\right]\to\mathbb{R}{% endequation %} ומגדירים סכום רימן-סטילטיס בתור {% equation %}S_{P}=\sum_{i=1}^{n}f\left(t_{i}^{*}\right)\left(g\left(t_{i}\right)-g\left(t_{i-1}\right)\right){% endequation %}. אם כל הדבר הזה מתכנס למשהו באותו מובן שבו סכום רימן התכנס, מסמנים את התוצאה ב-{% equation %}\int_{a}^{b}fdg{% endequation %}, כאשר כאן ה-{% equation %}dg{% endequation %} במקום {% equation %}dt{% endequation %} רומז לנו ש-{% equation %}g{% endequation %} הוא אינטגרטור. אינטגרל רימן ה"רגיל" מתקבל אם בוחרים {% equation %}g\left(t\right)=t{% endequation %}.

במקרים שבהם {% equation %}g{% endequation %} חלקה, אפשר להמיר את החישוב של אינטגרל רימן-סטילטיס בחישוב של אינטגרל רימן, באופן דומה למה שכבר ראינו אבל למרבה השמחה אפילו עוד יותר קל כי כאן <strong>אפשר</strong> להשתמש במשפט הערך הממוצע המקורי של לגראנז', על {% equation %}g{% endequation %} עצמה, ולקבל {% equation %}g\left(t_{i}\right)-g\left(t_{i-1}\right)=g^{\prime}\left(t_{i}^{*}\right)\Delta t_{i}{% endequation %} לכל תת-קטע. מרגע שיש לנו את זה, זו פשוט חזרה על ההוכחה שכבר ראינו קודם ש-{% equation %}\int_{a}^{b}fdg=\int_{a}^{b}f\left(t\right)g^{\prime}\left(t\right)dt{% endequation %} (שימו לב שכאן הפרמטר של {% equation %}f{% endequation %} הוא פשוט {% equation %}t{% endequation %}; הוא לא {% equation %}g\left(t\right){% endequation %} כמו שקורה באינטגרל קווי).

אינטגרל מרוכב מוגדר בצורה דומה מאוד. יש לנו את העקומה {% equation %}\gamma:\left[a,b\right]\to\mathbb{C}{% endequation %} שממלאת את התפקיד של {% equation %}g{% endequation %}, אבל במקום לסכום "סתם" ערכים של {% equation %}f{% endequation %}, אנחנו סוכמים ערכים של {% equation %}f\left(\gamma\left(t\right)\right){% endequation %}, כלומר אנחנו מסתכלים על משהו דמוי רימן-סטילטיס עבור אינטגרטור {% equation %}\gamma{% endequation %} והפונקציה {% equation %}f\circ\gamma{% endequation %}. מכיוון שאנחנו כבר ממש ממש ממש בסוף, בואו עוד פעם אחת אחרונה ודי נעשה את הכל פורמלי, כדי שלא יהיו לי יותר דאגות.

אם כן: נתונה לנו פונקציה {% equation %}f:\mathbb{C}\to\mathbb{C}{% endequation %} ועקומה {% equation %}z:\left[a,b\right]\to\mathbb{C}{% endequation %}. אנחנו לוקחים חלוקה {% equation %}P{% endequation %} של {% equation %}\left[a,b\right]{% endequation %}, ולכל בחירת נקודות {% equation %}t_{i}^{*}{% endequation %} אנחנו בונים את הסכום {% equation %}S_{P}^{z}=\sum_{i=1}^{n}f\left(z\left(t_{i}^{*}\right)\right)\left(z\left(t_{i}\right)-z\left(t_{i-1}\right)\right){% endequation %}. הפעם זה סכום של <strong>מספרים מרוכבים</strong>; גם {% equation %}f\left(z\left(t_{i}^{*}\right)\right){% endequation %} וגם {% equation %}\left(z\left(t_{i}\right)-z\left(t_{i-1}\right)\right){% endequation %} הם מספרים מרוכבים. הגדרת ההתכנסות נשארת זהה: אם קיים {% equation %}I\in\mathbb{C}{% endequation %} כך שלכל {% equation %}\varepsilon>0{% endequation %} קיים {% equation %}\delta{% endequation %} כך שלכל חלוקה {% equation %}P{% endequation %} עם {% equation %}\lambda\left(P\right)<\delta{% endequation %} וכל בחירת נקודות עבור חלוקה כזו, מתקיים {% equation %}\left|S_{P}^{z}-I\right|<\varepsilon{% endequation %}, אז אומרים ש-{% equation %}\int_{C}f\left(z\right)dz=I{% endequation %}. הפעם הערך המוחלט ב-{% equation %}\left|S_{P}^{z}-I\right|{% endequation %} הוא פונקציית הערך המוחלט של מספרים מרוכבים: {% equation %}\left|a+bi\right|=\sqrt{a^{2}+b^{2}}{% endequation %}.

אני רוצה להוכיח שתחת ההגדרה הזו מתקיים השוויון {% equation %}\int_{C}f\left(z\right)dz=\int_{a}^{b}f\left(z\left(t\right)\right)z^{\prime}\left(t\right)dt{% endequation %}, אבל מה בעצם הולך באגף ימין? זה לא אינטגרל רימן רגיל. אמנם, <strong>המשתנה</strong> של הפונקציה שבאינטגרל הוא מספר ממשי, {% equation %}t\in\left[a,b\right]{% endequation %}, אבל הפונקציות עצמן הן עדיין מרוכבות ולא דיברתי על אינטגרלים של פונקציות מרוכבות עם משתנה ממשי. למרבה המזל, זה ממש פשוט: כל פונקציה מרוכבת {% equation %}g:\left[a,b\right]\to\mathbb{C}{% endequation %} אפשר להציג בתור {% equation %}g\left(t\right)=x\left(t\right)+iy\left(t\right){% endequation %} כאשר {% equation %}x,y:\mathbb{R}\to\mathbb{R}{% endequation %} הן פונקציות ממשיות, ואז אפשר להגדיר 

{% equation %}\int_{a}^{b}g\left(t\right)dt=\int_{a}^{b}x\left(t\right)dt+i\int_{a}^{b}y\left(t\right)dt{% endequation %}

כאשר כאן באגף ימין יש שני אינטגרלים רגילים של פונקציות ממשיות. זה גם מה שעשינו כשלקחנו אינטגרל של פונקציה וקטורית, כשהתעסקנו באינטגרלים קוויים. דבר דומה קורה גם עבור נגזרות:

{% equation %}g^{\prime}\left(t\right)=\lim_{h\to0}\frac{g\left(t+h\right)-g\left(t\right)}{h}=\lim_{h\to0}\left(\frac{x\left(t+h\right)-x\left(t\right)}{h}+i\frac{y\left(t+h\right)-y\left(t\right)}{h}\right)=x^{\prime}\left(t\right)+iy^{\prime}\left(t\right){% endequation %}

אז זה מה שנצטרך לעשות כדי להוכיח את השוויון {% equation %}\int_{C}f\left(z\right)dz=\int_{a}^{b}f\left(z\left(t\right)\right)z^{\prime}\left(t\right)dt{% endequation %} - לפרק לשני חלקים שאחד ממשי והשני מדומה, ולהוציא את ה-{% equation %}i{% endequation %} החוצה. אז בואו ונכתוב

{% equation %}f\left(z\left(t\right)\right)=u\left(t\right)+iv\left(t\right){% endequation %}

{% equation %}z\left(t\right)=x\left(t\right)+iy\left(t\right){% endequation %}

{% equation %}z^{\prime}\left(t\right)=x^{\prime}\left(t\right)+iy^{\prime}\left(t\right){% endequation %}

ועכשיו:

{% equation %}\int_{a}^{b}f\left(z\left(t\right)\right)z^{\prime}\left(t\right)dt=\int_{a}^{b}\left[u\left(t\right)+iv\left(t\right)\right]\left[x^{\prime}\left(t\right)+iy^{\prime}\left(t\right)\right]dt{% endequation %}

{% equation %}=\int_{a}^{b}\left[u\left(t\right)x^{\prime}\left(t\right)-v\left(t\right)y^{\prime}\left(t\right)\right]dt+i\int_{a}^{b}\left[u\left(t\right)y^{\prime}\left(t\right)+v\left(t\right)x^{\prime}\left(t\right)\right]dt{% endequation %}

זה נראה כמו סמטוחה אחת גדולה, אבל אין עם זה בעיה - כל עוד גם {% equation %}\int_{C}f\left(z\right)dz{% endequation %} מתכנן להיראות כמו סמטוחה אחת גדולה דומה כשנסיים איתו. בואו נכתוב סכום רימן כללי עבורו:

{% equation %}\sum_{i=1}^{n}f\left(z\left(t_{i}^{*}\right)\right)\left(z\left(t_{i}\right)-z\left(t_{i-1}\right)\right){% endequation %}

בינתיים אני <strong>לא יכול</strong> לפשט את {% equation %}z\left(t_{i}\right)-z\left(t_{i-1}\right){% endequation %} כי אין לי אנלוג ישיר למשפט לגראנז' עבור פונקציות עם טווח מרוכב. אז מה אני אעשה? אני אלך עם הראש בקיר ואציב דברים:

{% equation %}\sum_{i=1}^{n}f\left(z\left(t_{i}^{*}\right)\right)\left(z\left(t_{i}\right)-z\left(t_{i-1}\right)\right)=\sum_{i=1}^{n}\left[\left(u\left(t_{i}^{*}\right)+iv\left(t_{i}^{*}\right)\right)\left(x\left(t_{i}\right)+iy\left(t_{i}\right)-x\left(t_{i-1}\right)-iy\left(t_{i-1}\right)\right)\right]={% endequation %}

{% equation %}=\sum_{i=1}^{n}\left[u\left(t_{i}^{*}\right)\left(x\left(t_{i}\right)-x\left(t_{i-1}\right)\right)-v\left(t_{i}^{*}\right)\left(y\left(t_{i}\right)-y\left(t_{i-1}\right)\right)\right]+{% endequation %}

{% equation %}+i\sum_{i=1}^{n}\left[u\left(t_{i}^{*}\right)\left(y\left(t_{i}\right)-y\left(t_{i-1}\right)\right)+v\left(t_{i}^{*}\right)\left(x\left(t_{i}\right)-x\left(t_{i-1}\right)\right)\right]{% endequation %}

בינתיים זה נראה טוב, אבל שוד ושבר - יש לנו גם את הביטוי {% equation %}x\left(t_{i}\right)-x\left(t_{i-1}\right){% endequation %} וגם את הביטוי {% equation %}y\left(t_{i}\right)-y\left(t_{i-1}\right){% endequation %} ולא נוכל להשתמש בלגראנז' סימולטנית על שניהם. לכן אני אנקוט בטקטיקה אחרת - אני אפצל את הכל <strong>עוד פעם</strong> ואקבל מהאינטגרל המקורי סכום של ארבעה אינטגרלים, ומסכום הרימן המקורי ארבעה סכומים. בואו נכתוב אותם זה לצד זה:

<ul> <li>{% equation %}\int_{a}^{b}u\left(t\right)x^{\prime}\left(t\right)dt{% endequation %} אל מול {% equation %}\sum_{i=1}^{n}u\left(t_{i}^{*}\right)\left(x\left(t_{i}\right)-x\left(t_{i-1}\right)\right){% endequation %}</li>


<li>{% equation %}-\int_{a}^{b}v\left(t\right)y^{\prime}\left(t\right)dt{% endequation %} אל מול {% equation %}-\sum_{i=1}^{n}v\left(t_{i}^{*}\right)\left(y\left(t_{i}\right)-y\left(t_{i-1}\right)\right){% endequation %}</li>


<li>{% equation %}i\int_{a}^{b}u\left(t\right)y^{\prime}\left(t\right)dt{% endequation %} אל מול {% equation %}i\sum_{i=1}^{n}u\left(t_{i}^{*}\right)\left(y\left(t_{i}\right)-y\left(t_{i-1}\right)\right){% endequation %}</li>


<li>{% equation %}i\int_{a}^{b}v\left(t\right)x^{\prime}\left(t\right)dt{% endequation %} אל מול {% equation %}i\sum_{i=1}^{n}v\left(t_{i}^{*}\right)\left(x\left(t_{i}\right)-x\left(t_{i-1}\right)\right){% endequation %}</li>

</ul>

עכשיו אני אוכיח שמתקיים{% equation %}\int_{C}f\left(z\right)dz=\int_{a}^{b}f\left(z\left(t\right)\right)z^{\prime}\left(t\right)dt{% endequation %} על פי ההגדרה של {% equation %}\int_{C}f\left(z\right)dz{% endequation %}. כלומר, ניקח {% equation %}\varepsilon>0{% endequation %} ונוכיח שקיימת {% equation %}\delta>0{% endequation %} כך שאם {% equation %}P{% endequation %} היא חלוקה כלשהי עם {% equation %}\lambda\left(P\right)<\delta{% endequation %}, אז {% equation %}\left|S_{P}^{z}-\int_{a}^{b}f\left(z\left(t\right)\right)z^{\prime}\left(t\right)dt\right|<\varepsilon{% endequation %}.

עכשיו, ראינו איך אפשר לפצל גם את {% equation %}S_{P}^{z}{% endequation %} וגם את {% equation %}\int_{a}^{b}f\left(z\left(t\right)\right)z^{\prime}\left(t\right)dt{% endequation %} לארבעה חלקים שמתאימים זה לזה בזוגות. לכן כדי לחסום את ההפרש, אני משתמש באי שוויון המשולש על פיצול לארבעה חלקים של כל אחד מהביטויים. בואו נראה איך דבר כזה נראה, סכמטית:

{% equation %}\left|\left(A_{1}+B_{1}+C_{1}+D_{1}\right)-\left(A_{2}+B_{2}+C_{2}+D_{2}\right)\right|\le\left|A_{1}-A_{2}\right|+\left|B_{1}-B_{2}\right|+\left|C_{1}-C_{2}\right|+\left|D_{1}-D_{2}\right|{% endequation %}

במקרה שלנו המקבילה ל-{% equation %}\left|A_{1}-A_{2}\right|{% endequation %} תהיה

{% equation %}\left|\int_{a}^{b}u\left(t\right)x^{\prime}\left(t\right)dt-\sum_{i=1}^{n}u\left(t_{i}^{*}\right)\left(x\left(t_{i}\right)-x\left(t_{i-1}\right)\right)\right|{% endequation %}

ודי ברור מה יהיו שאר המקבילות (ושהכפל ב-{% equation %}-1{% endequation %} או ב-{% equation %}i{% endequation %} לא משפיע; הערך המוחלט מעלים אותו).

עכשיו צריך לשים לב לנקודה עדינה: {% equation %}\int_{a}^{b}u\left(t\right)x^{\prime}\left(t\right)dt{% endequation %} נראה כמו החישוב של אינטגרל רימן-סטילטיס, וכבר ראינו ש-{% equation %}\int_{a}^{b}u\left(t\right)x^{\prime}\left(t\right)dt=\int_{a}^{b}u\left(t\right)dx{% endequation %}. המשמעות של השוויון הזה היא שסכומי רימן-סטילטיס של {% equation %}\int_{a}^{b}u\left(t\right)dx{% endequation %} מתקרבים אל {% equation %}\int_{a}^{b}u\left(t\right)x^{\prime}\left(t\right)dt{% endequation %}. כלומר, עבור {% equation %}\frac{\varepsilon}{4}{% endequation %} קיים {% equation %}\delta_{1}{% endequation %} כך שאם {% equation %}P{% endequation %} חלוקה עם {% equation %}\lambda\left(P\right)<\delta_{1}{% endequation %}, אז <strong>כל</strong> סכום רימן-סטילטיס ובפרט הסכום {% equation %}\sum_{i=1}^{n}u\left(t_{i}^{*}\right)\left(x\left(t_{i}\right)-x\left(t_{i-1}\right)\right){% endequation %} יהיה קרוב אל {% equation %}\int_{a}^{b}u\left(t\right)x^{\prime}\left(t\right)dt{% endequation %} עד כדי {% equation %}\frac{\varepsilon}{4}{% endequation %}.

באותו האופן אנחנו מוצאים {% equation %}\delta_{2},\delta_{3},\delta_{4}{% endequation %} עבור שלושת הביטויים האחרים, ואז מגדירים {% equation %}\delta=\min\left\{ \delta_{1},\delta_{2},\delta_{3},\delta_{4}\right\} {% endequation %}. עכשיו מובטח לנו שעבור כל חלוקה {% equation %}P{% endequation %} שמקיימת {% equation %}\lambda\left(P\right)<\delta{% endequation %} וכל סכום רימן {% equation %}S_{P}^{z}{% endequation %} שמתאים לה, {% equation %}\left|S_{P}^{z}-\int_{a}^{b}f\left(z\left(t\right)\right)z^{\prime}\left(t\right)dt\right|<\varepsilon{% endequation %}. זה מסיים את ההוכחה, ומרגיע אותי סופית. עכשיו מבחינתי זה בסדר גמור להגדיר אינטגרל מרוכב על ידי {% equation %}\int_{C}f\left(z\right)dz=\int_{a}^{b}f\left(z\left(t\right)\right)z^{\prime}\left(t\right)dt{% endequation %} ותו לא. סוף טוב הכל טוב! 
